@@ -34,7 +34,7 @@ class CollapsedForwardingMsg
     int sender; ///< kid ID of sending process
 
     /// transients index, so that workers can find [private] entries to sync
-    sfileno xitIndex; 
+    sfileno xitIndex;
 };
 
 // CollapsedForwarding
@@ -54,7 +54,7 @@ CollapsedForwarding::Broadcast(const StoreEntry &e)
         return;
 
     if (!e.mem_obj || e.mem_obj->xitTable.index < 0 ||
-        !Store::Root().transientReaders(e)) {
+            !Store::Root().transientReaders(e)) {
         debugs(17, 7, "nobody reads " << e);
         return;
     }
@@ -6,8 +6,8 @@
 #ifndef SQUID_COLLAPSED_FORWARDING_H
 #define SQUID_COLLAPSED_FORWARDING_H
 
-#include "ipc/Queue.h"
 #include "ipc/forward.h"
+#include "ipc/Queue.h"
 #include "typedefs.h"
 
 #include <memory>
@@ -659,7 +659,7 @@ diskerRead(IpcIoMsg &ipcIo)
     }
 }
 
-/// Tries to write buffer to disk (a few times if needed); 
+/// Tries to write buffer to disk (a few times if needed);
 /// sets ipcIo results, but does no cleanup. The caller must cleanup.
 static void
 diskerWriteAttempts(IpcIoMsg &ipcIo)
@@ -75,7 +75,8 @@ MemObject::inUseCount()
 }
 
 const char *
-MemObject::storeId() const {
+MemObject::storeId() const
+{
     if (!storeId_.size()) {
         debugs(20, DBG_IMPORTANT, "Bug: Missing MemObject::storeId value");
         dump();
@@ -85,12 +86,14 @@ MemObject::storeId() const {
 }
 
 const char *
-MemObject::logUri() const {
+MemObject::logUri() const
+{
     return logUri_.size() ? logUri_.termedBuf() : storeId();
 }
 
 bool
-MemObject::hasUris() const {
+MemObject::hasUris() const
+{
     return storeId_.size();
 }
 
@@ -100,7 +103,7 @@ MemObject::setUris(char const *aStoreId, char const *aLogUri, const HttpRequestM
     storeId_ = aStoreId;
 
     // fast pointer comparison for a common storeCreateEntry(url,url,...) case
-    if (!aLogUri || aLogUri == aStoreId) 
+    if (!aLogUri || aLogUri == aStoreId)
         logUri_.clean(); // use storeId_ by default to minimize copying
     else
         logUri_ = aLogUri;
@@ -334,7 +337,7 @@ bool
 MemObject::readAheadPolicyCanRead() const
 {
     const bool canRead = endOffset() - getReply()->hdr_sz <
-        lowestMemReaderOffset() + Config.readAheadGap;
+                         lowestMemReaderOffset() + Config.readAheadGap;
 
     if (!canRead) {
         debugs(19, 9, "no: " << endOffset() << '-' << getReply()->hdr_sz <<
@@ -143,7 +143,8 @@ class MemObject
     typedef enum { ioUndecided, ioWriting, ioReading, ioDone } Io;
 
     /// State of an entry with regards to the [shared] in-transit table.
-    class XitTable {
+    class XitTable
+    {
     public:
         XitTable(): index(-1), io(ioUndecided) {}
 
@@ -153,13 +154,14 @@ class MemObject
     XitTable xitTable; ///< current [shared] memory caching state for the entry
 
     /// State of an entry with regards to the [shared] memory caching.
-    class MemCache {
+    class MemCache
+    {
     public:
         MemCache(): index(-1), offset(0), io(ioUndecided) {}
 
         int32_t index; ///< entry position inside the memory cache
         int64_t offset; ///< bytes written/read to/from the memory cache so far
-        
+
         Io io; ///< current I/O state
     };
     MemCache memCache; ///< current [shared] memory caching state for the entry
@@ -28,7 +28,6 @@ static const char *SpaceLabel = "cache_mem_space";
 // used except for a positivity test. A unique value is handy for debugging.
 static const uint32_t SpacePoolId = 510716;
 
-
 MemStore::MemStore(): map(NULL), lastWritingSlice(-1)
 {
 }
@@ -47,18 +46,18 @@ MemStore::init()
 
     // check compatibility with the disk cache, if any
     if (Config.cacheSwap.n_configured > 0) {
-    const int64_t diskMaxSize = Store::Root().maxObjectSize();
-    const int64_t memMaxSize = maxObjectSize();
-    if (diskMaxSize == -1) {
-        debugs(20, DBG_IMPORTANT, "WARNING: disk-cache maximum object size "
-               "is unlimited but mem-cache maximum object size is " <<
-               memMaxSize / 1024.0 << " KB");
-    } else if (diskMaxSize > memMaxSize) {
-        debugs(20, DBG_IMPORTANT, "WARNING: disk-cache maximum object size "
-               "is too large for mem-cache: " <<
-               diskMaxSize / 1024.0 << " KB > " <<
-               memMaxSize / 1024.0 << " KB");
-    }
+        const int64_t diskMaxSize = Store::Root().maxObjectSize();
+        const int64_t memMaxSize = maxObjectSize();
+        if (diskMaxSize == -1) {
+            debugs(20, DBG_IMPORTANT, "WARNING: disk-cache maximum object size "
+                   "is unlimited but mem-cache maximum object size is " <<
+                   memMaxSize / 1024.0 << " KB");
+        } else if (diskMaxSize > memMaxSize) {
+            debugs(20, DBG_IMPORTANT, "WARNING: disk-cache maximum object size "
+                   "is too large for mem-cache: " <<
+                   diskMaxSize / 1024.0 << " KB > " <<
+                   memMaxSize / 1024.0 << " KB");
+        }
     }
 
     freeSlots = shm_old(Ipc::Mem::PageStack)(SpaceLabel);
@@ -136,7 +135,7 @@ uint64_t
 MemStore::currentSize() const
 {
     return Ipc::Mem::PageLevel(Ipc::Mem::PageId::cachePage) *
-        Ipc::Mem::PageSize();
+           Ipc::Mem::PageSize();
 }
 
 uint64_t
@@ -224,7 +223,7 @@ MemStore::anchorCollapsed(StoreEntry &collapsed, bool &inSync)
 
     sfileno index;
     const Ipc::StoreMapAnchor *const slot = map->openForReading(
-        reinterpret_cast<cache_key*>(collapsed.key), index);
+                                                reinterpret_cast<cache_key*>(collapsed.key), index);
     if (!slot)
         return false;
 
@@ -238,10 +237,10 @@ MemStore::updateCollapsed(StoreEntry &collapsed)
 {
     assert(collapsed.mem_obj);
 
-    const sfileno index = collapsed.mem_obj->memCache.index; 
+    const sfileno index = collapsed.mem_obj->memCache.index;
 
     // already disconnected from the cache, no need to update
-    if (index < 0) 
+    if (index < 0)
         return true;
 
     if (!map)
@@ -318,8 +317,8 @@ MemStore::copyFromShm(StoreEntry &e, const sfileno index, const Ipc::StoreMapAnc
                wasEof << " wasSize " << wasSize << " <= " <<
                anchor.basics.swap_file_sz << " sliceOffset " << sliceOffset <<
                " mem.endOffset " << e.mem_obj->endOffset());
- 
-       if (e.mem_obj->endOffset() < sliceOffset + wasSize) {
+
+        if (e.mem_obj->endOffset() < sliceOffset + wasSize) {
             // size of the slice data that we already copied
             const size_t prefixSize = e.mem_obj->endOffset() - sliceOffset;
             assert(prefixSize <= wasSize);
@@ -343,7 +342,7 @@ MemStore::copyFromShm(StoreEntry &e, const sfileno index, const Ipc::StoreMapAnc
             if (wasSize >= slice.size) { // did not grow since we started copying
                 sliceOffset += wasSize;
                 sid = slice.next;
-			}
+            }
         } else if (wasSize >= slice.size) { // did not grow
             break;
         }
@@ -385,7 +384,7 @@ MemStore::copyFromShmSlice(StoreEntry &e, const StoreIOBuffer &buf, bool eof)
         // XXX: have to copy because httpMsgParseStep() requires 0-termination
         MemBuf mb;
         mb.init(buf.length+1, buf.length+1);
-        mb.append(buf.data, buf.length);        
+        mb.append(buf.data, buf.length);
         mb.terminate();
         const int result = rep->httpMsgParseStep(mb.buf, buf.length, eof);
         if (result > 0) {
@@ -494,7 +493,7 @@ MemStore::copyToShm(StoreEntry &e)
     // not knowing when the wait is over
     if (EBIT_TEST(e.flags, ENTRY_FWD_HDR_WAIT)) {
         debugs(20, 5, "postponing copying " << e << " for ENTRY_FWD_HDR_WAIT");
-        return;         
+        return;
     }
 
     assert(map);
@@ -536,9 +535,9 @@ MemStore::copyToShm(StoreEntry &e)
             slice.next = lastWritingSlice = reserveSapForWriting(page);
             map->extras(lastWritingSlice).page = page;
             debugs(20, 7, "entry " << index << " new slice: " << lastWritingSlice);
-         }
+        }
 
-         copyToShmSlice(e, anchor);
+        copyToShmSlice(e, anchor);
     }
 
     debugs(20, 7, "mem-cached available " << eSize << " bytes of " << e);
@@ -594,7 +593,7 @@ MemStore::reserveSapForWriting(Ipc::Mem::PageId &page)
             freeSlots->push(slot);
         }
     }
-        
+
     // catch free slots delivered to noteFreeMapSlice()
     assert(!waitingFor);
     waitingFor.slot = &slot;
@@ -651,7 +650,7 @@ MemStore::write(StoreEntry &e)
             return;
         }
         break;
-  
+
     case MemObject::ioDone:
     case MemObject::ioReading:
         return; // we should not write in all of the above cases
@@ -667,8 +666,7 @@ MemStore::write(StoreEntry &e)
         else
             CollapsedForwarding::Broadcast(e);
         return;
-    }
-    catch (const std::exception &x) { // TODO: should we catch ... as well?
+    } catch (const std::exception &x) { // TODO: should we catch ... as well?
         debugs(20, 2, "mem-caching error writing entry " << e << ": " << x.what());
         // fall through to the error handling code
     }
@@ -712,7 +710,7 @@ MemStore::unlink(StoreEntry &e)
         // the entry may have been loaded and then disconnected from the cache
         map->freeEntryByKey(reinterpret_cast<cache_key*>(e.key));
     }
-        
+
     e.destroyMemObject(); // XXX: but it may contain useful info such as a client list. The old code used to do that though, right?
 }
 
@@ -834,8 +832,8 @@ void MemStoreRr::create(const RunnerRegistry &)
 
     Must(!spaceOwner);
     spaceOwner = shm_new(Ipc::Mem::PageStack)(SpaceLabel, SpacePoolId,
-                                              entryLimit,
-                                              sizeof(Ipc::Mem::PageId));
+                 entryLimit,
+                 sizeof(Ipc::Mem::PageId));
     Must(!mapOwner);
     mapOwner = MemStoreMap::Init(MapLabel, entryLimit);
 }
@@ -83,7 +83,8 @@ class MemStore: public Store, public Ipc::StoreMapCleaner
     sfileno lastWritingSlice;
 
     /// temporary storage for slot and page ID pointers; for the waiting cache
-    class SlotAndPage {
+    class SlotAndPage
+    {
     public:
         SlotAndPage(): slot(NULL), page(NULL) {}
         bool operator !() const { return !slot && !page; }
@@ -183,7 +183,7 @@ Transients::copyFromShm(const sfileno index)
 
     // create a brand new store entry and initialize it with stored info
     StoreEntry *e = storeCreatePureEntry(extras.url, extras.url,
-                                     extras.reqFlags, extras.reqMethod);
+                                         extras.reqFlags, extras.reqMethod);
 
     assert(e->mem_obj);
     e->mem_obj->method = extras.reqMethod;
@@ -230,7 +230,7 @@ Transients::findCollapsed(const sfileno index)
 
 void
 Transients::startWriting(StoreEntry *e, const RequestFlags &reqFlags,
-                const HttpRequestMethod &reqMethod)
+                         const HttpRequestMethod &reqMethod)
 {
     assert(e);
     assert(e->mem_obj);
@@ -239,14 +239,14 @@ Transients::startWriting(StoreEntry *e, const RequestFlags &reqFlags,
     if (!map) {
         debugs(20, 5, "No map to add " << *e);
         return;
-	}
+    }
 
     sfileno index = 0;
     Ipc::StoreMapAnchor *slot = map->openForWriting(reinterpret_cast<const cache_key *>(e->key), index);
     if (!slot) {
         debugs(20, 5, "collision registering " << *e);
         return;
-	}
+    }
 
     try {
         if (copyToShm(*e, index, reqFlags, reqMethod)) {
@@ -256,14 +256,13 @@ Transients::startWriting(StoreEntry *e, const RequestFlags &reqFlags,
             map->startAppending(index);
             // keep write lock -- we will be supplying others with updates
             return;
-		}
+        }
         // fall through to the error handling code
-	} 
-    catch (const std::exception &x) { // TODO: should we catch ... as well?
+    } catch (const std::exception &x) { // TODO: should we catch ... as well?
         debugs(20, 2, "error keeping entry " << index <<
                ' ' << *e << ": " << x.what());
         // fall through to the error handling code
-	}
+    }
 
     map->abortWriting(index);
 }
@@ -280,7 +279,7 @@ Transients::copyToShm(const StoreEntry &e, const sfileno index,
     const size_t urlLen = strlen(url);
     Must(urlLen < sizeof(extras.url)); // we have space to store it all, plus 0
     strncpy(extras.url, url, sizeof(extras.url));
-	extras.url[urlLen] = '\0';
+    extras.url[urlLen] = '\0';
 
     extras.reqFlags = reqFlags;
 
@@ -2167,8 +2167,8 @@ clientReplyContext::createStoreEntry(const HttpRequestMethod& m, RequestFlags re
     // TODO: every must-revalidate and similar request MUST reach the origin,
     // but do we have to prohibit others from collapsing on that request?
     if (Config.onoff.collapsed_forwarding && reqFlags.cachable &&
-        !reqFlags.needValidation &&
-        (m == Http::METHOD_GET || m == Http::METHOD_HEAD)) {
+            !reqFlags.needValidation &&
+            (m == Http::METHOD_GET || m == Http::METHOD_HEAD)) {
         // make the entry available for future requests now
         Store::Root().allowCollapsing(e, reqFlags, m);
     }
@@ -496,11 +496,11 @@ Format::Format::assemble(MemBuf &mb, const AccessLogEntry::Pointer &al, int logS
         break;
 
         case LFT_TIME_START: {
-            int precision = fmt->widthMax >=0 ? fmt->widthMax : 3;
+            int precision = fmt->widthMax >=0 ? fmt->widthMax :3;
             snprintf(tmp, sizeof(tmp), "%0*" PRId64 ".%0*d", fmt->zero && (fmt->widthMin - precision - 1 >= 0) ? fmt->widthMin - precision - 1 : 0, (int64_t)al->cache.start_time.tv_sec, precision, (int)(al->cache.start_time.tv_usec / fmt->divisor));
             out = tmp;
         }
-            break;
+        break;
 
         case LFT_TIME_TO_HANDLE_REQUEST:
             outint = al->cache.msec;
@@ -22,11 +22,13 @@ class DbCellHeader
     /* members below are not meaningful if empty() */
 
     /// whether this slot is not corrupted
-    bool sane(const size_t slotSize, int slotLimit) const { return
-       0 <= firstSlot && firstSlot < slotLimit &&
-       -1 <= nextSlot && nextSlot < slotLimit &&
-       version > 0 &&
-       0 < payloadSize && payloadSize <= slotSize - sizeof(DbCellHeader); }
+    bool sane(const size_t slotSize, int slotLimit) const {
+        return
+            0 <= firstSlot && firstSlot < slotLimit &&
+            -1 <= nextSlot && nextSlot < slotLimit &&
+            version > 0 &&
+            0 < payloadSize && payloadSize <= slotSize - sizeof(DbCellHeader);
+    }
 
     uint64_t key[2]; ///< StoreEntry key
     uint64_t entrySize; ///< total entry content size or zero if still unknown
@@ -119,10 +119,10 @@ Rock::IoState::read_(char *buf, size_t len, off_t coreOff, STRCB *cb, void *data
 
     offset_ = coreOff;
     len = min(len,
-        static_cast<size_t>(objOffset + currentReadableSlice().size - coreOff));
+              static_cast<size_t>(objOffset + currentReadableSlice().size - coreOff));
     const uint64_t diskOffset = dir->diskOffset(sidCurrent);
     theFile->read(new ReadRequest(::ReadRequest(buf,
-        diskOffset + sizeof(DbCellHeader) + coreOff - objOffset, len), this));
+                                  diskOffset + sizeof(DbCellHeader) + coreOff - objOffset, len), this));
 }
 
 void
@@ -137,7 +137,6 @@ Rock::IoState::callReaderBack(const char *buf, int rlen)
         callb(cbdata, buf, rlen, this);
 }
 
-
 /// wraps tryWrite() to handle deep write failures centrally and safely
 bool
 Rock::IoState::write(char const *buf, size_t size, off_t coreOff, FREE *dtor)
@@ -154,7 +153,7 @@ Rock::IoState::write(char const *buf, size_t size, off_t coreOff, FREE *dtor)
     }
 
     // careful: 'this' might be gone here
- 
+
     if (dtor)
         (dtor)(const_cast<char*>(buf)); // cast due to a broken API?
 
@@ -165,7 +164,7 @@ Rock::IoState::write(char const *buf, size_t size, off_t coreOff, FREE *dtor)
  * Possibly send data to be written to disk:
  * We only write data when full slot is accumulated or when close() is called.
  * We buffer, in part, to avoid forcing OS to _read_ old unwritten portions of
- * the slot when the write does not end at the page or sector boundary. 
+ * the slot when the write does not end at the page or sector boundary.
  */
 void
 Rock::IoState::tryWrite(char const *buf, size_t size, off_t coreOff)
@@ -276,7 +275,7 @@ Rock::IoState::writeBufToDisk(const SlotId sidNext, bool eof)
 
     WriteRequest *const r = new WriteRequest(
         ::WriteRequest(static_cast<char*>(wBuf), diskOffset, theBuf.size,
-            memFreeBufFunc(wBufCap)), this);
+                       memFreeBufFunc(wBufCap)), this);
     r->sidCurrent = sidCurrent;
     r->sidNext = sidNext;
     r->eof = eof;
@@ -55,7 +55,7 @@ class IoState: public ::StoreIOState
     void writeToDisk(const SlotId nextSlot);
     void writeBufToDisk(const SlotId nextSlot, const bool eof);
     SlotId reserveSlotForWriting();
-    
+
     void callBack(int errflag);
 
     Rock::SwapDir::Pointer dir; ///< swap dir that initiated I/O
@@ -68,15 +68,16 @@ CBDATA_NAMESPACED_CLASS_INIT(Rock, Rebuild);
  *  negligible performance impact but saves us from high-damage bugs.
  */
 
-
-namespace Rock {
+namespace Rock
+{
 
 /// maintains information about the store entry being loaded from disk
 /// used for identifying partially stored/loaded entries
-class LoadingEntry {
+class LoadingEntry
+{
 public:
     LoadingEntry(): size(0), version(0), state(leEmpty), anchored(0),
-        mapped(0), freed(0), more(-1) {}
+            mapped(0), freed(0), more(-1) {}
 
     /* store entry-level information indexed by sfileno */
     uint64_t size; ///< payload seen so far
@@ -96,7 +97,6 @@ class LoadingEntry {
 
 } /* namespace Rock */
 
-
 Rock::Rebuild::Rebuild(SwapDir *dir): AsyncJob("Rock::Rebuild"),
         sd(dir),
         entries(NULL),
@@ -169,7 +169,7 @@ bool
 Rock::Rebuild::doneAll() const
 {
     return dbOffset >= dbSize && validationPos >= dbEntryLimit &&
-        AsyncJob::doneAll();
+           AsyncJob::doneAll();
 }
 
 void
@@ -275,7 +275,7 @@ Rock::Rebuild::importEntry(Ipc::StoreMapAnchor &anchor, const sfileno fileno, co
     cache_key key[SQUID_MD5_DIGEST_LENGTH];
     StoreEntry loadedE;
     const uint64_t knownSize = header.entrySize > 0 ?
-        header.entrySize : anchor.basics.swap_file_sz.get();
+                               header.entrySize : anchor.basics.swap_file_sz.get();
     if (!storeRebuildParseEntry(buf, loadedE, key, counts, knownSize))
         return false;
 
@@ -487,12 +487,10 @@ Rock::Rebuild::addSlotToEntry(const sfileno fileno, const SlotId slotId, const D
         assert(anchor.basics.swap_file_sz != static_cast<uint64_t>(-1));
         // perhaps we loaded a later slot (with entrySize) earlier
         totalSize = anchor.basics.swap_file_sz;
-    } else
-    if (totalSize && !anchor.basics.swap_file_sz) {
+    } else if (totalSize && !anchor.basics.swap_file_sz) {
         anchor.basics.swap_file_sz = totalSize;
         assert(anchor.basics.swap_file_sz != static_cast<uint64_t>(-1));
-    } else
-    if (totalSize != anchor.basics.swap_file_sz) {
+    } else if (totalSize != anchor.basics.swap_file_sz) {
         le.state = LoadingEntry::leCorrupted;
         freeBadEntry(fileno, "size mismatch");
         return;
@@ -539,7 +537,7 @@ void
 Rock::Rebuild::startNewEntry(const sfileno fileno, const SlotId slotId, const DbCellHeader &header)
 {
     // If some other from-disk entry is/was using this slot as its inode OR
-    // if some other from-disk entry is/was using our inode slot, then the 
+    // if some other from-disk entry is/was using our inode slot, then the
     // entries are conflicting. We cannot identify other entries, so we just
     // remove ours and hope that the others were/will be handled correctly.
     const LoadingEntry &slice = entries[slotId];
@@ -578,8 +576,8 @@ Rock::Rebuild::sameEntry(const sfileno fileno, const DbCellHeader &header) const
     const LoadingEntry &le = entries[fileno];
     // any order will work, but do fast comparisons first:
     return le.version == header.version &&
-        anchor.start == static_cast<Ipc::StoreMapSliceId>(header.firstSlot) &&
-        anchor.sameKey(reinterpret_cast<const cache_key*>(header.key));
+           anchor.start == static_cast<Ipc::StoreMapSliceId>(header.firstSlot) &&
+           anchor.sameKey(reinterpret_cast<const cache_key*>(header.key));
 }
 
 /// is the new header consistent with information already loaded?
@@ -637,7 +635,7 @@ Rock::Rebuild::useNewSlot(const SlotId slotId, const DbCellHeader &header)
 
     LoadingEntry &le = entries[fileno];
     debugs(47,9, "entry " << fileno << " state: " << le.state << ", inode: " <<
-            header.firstSlot << ", size: " << header.payloadSize);
+           header.firstSlot << ", size: " << header.payloadSize);
 
     switch (le.state) {
 
@@ -31,9 +31,9 @@
 
 const int64_t Rock::SwapDir::HeaderSize = 16*1024;
 
-Rock::SwapDir::SwapDir(): ::SwapDir("rock"), 
-    slotSize(HeaderSize), filePath(NULL), map(NULL), io(NULL),
-    waitingForPage(NULL)
+Rock::SwapDir::SwapDir(): ::SwapDir("rock"),
+        slotSize(HeaderSize), filePath(NULL), map(NULL), io(NULL),
+        waitingForPage(NULL)
 {
 }
 
@@ -88,7 +88,7 @@ Rock::SwapDir::anchorCollapsed(StoreEntry &collapsed, bool &inSync)
 
     sfileno filen;
     const Ipc::StoreMapAnchor *const slot = map->openForReading(
-        reinterpret_cast<cache_key*>(collapsed.key), filen);
+                                                reinterpret_cast<cache_key*>(collapsed.key), filen);
     if (!slot)
         return false;
 
@@ -164,7 +164,7 @@ void Rock::SwapDir::disconnect(StoreEntry &e)
     // especially since we may switch from writing to reading. This code relies
     // on Rock::IoState::writeableAnchor_ being set when we locked for writing.
     if (e.mem_obj && e.mem_obj->swapout.sio != NULL &&
-        dynamic_cast<IoState&>(*e.mem_obj->swapout.sio).writeableAnchor_) {
+            dynamic_cast<IoState&>(*e.mem_obj->swapout.sio).writeableAnchor_) {
         map->abortWriting(e.swap_filen);
         e.swap_dirn = -1;
         e.swap_filen = -1;
@@ -183,7 +183,7 @@ uint64_t
 Rock::SwapDir::currentSize() const
 {
     const uint64_t spaceSize = !freeSlots ?
-        maxSize() : (slotSize * freeSlots->size());
+                               maxSize() : (slotSize * freeSlots->size());
     // everything that is not free is in use
     return maxSize() - spaceSize;
 }
@@ -278,7 +278,8 @@ Rock::SwapDir::create()
 
 // report Rock DB creation error and exit
 void
-Rock::SwapDir::createError(const char *const msg) {
+Rock::SwapDir::createError(const char *const msg)
+{
     debugs(47, DBG_CRITICAL, "ERROR: Failed to initialize Rock Store db in " <<
            filePath << "; " << msg << " error: " << xstrerror());
     fatal("Rock Store db creation error");
@@ -819,12 +820,12 @@ Rock::SwapDir::writeCompleted(int errflag, size_t rlen, RefCount< ::WriteRequest
             map->writeableSlice(sio.swap_filen, request->sidCurrent);
         slice.size = request->len - sizeof(DbCellHeader);
         slice.next = request->sidNext;
-        
+
         if (request->eof) {
             assert(sio.e);
             assert(sio.writeableAnchor_);
             sio.e->swap_file_sz = sio.writeableAnchor_->basics.swap_file_sz =
-                sio.offset_;
+                                      sio.offset_;
 
             // close, the entry gets the read lock
             map->closeForWriting(sio.swap_filen, true);
@@ -984,15 +985,17 @@ Rock::SwapDir::statfs(StoreEntry &e) const
 }
 
 const char *
-Rock::SwapDir::inodeMapPath() const {
+Rock::SwapDir::inodeMapPath() const
+{
     static String inodesPath;
     inodesPath = path;
     inodesPath.append("_inodes");
     return inodesPath.termedBuf();
 }
 
 const char *
-Rock::SwapDir::freeSlotsPath() const {
+Rock::SwapDir::freeSlotsPath() const
+{
     static String spacesPath;
     spacesPath = path;
     spacesPath.append("_spaces");
@@ -5,9 +5,9 @@
 #include "DiskIO/IORequestor.h"
 #include "fs/rock/RockDbCell.h"
 #include "fs/rock/RockForward.h"
-#include "ipc/StoreMap.h"
 #include "ipc/mem/Page.h"
 #include "ipc/mem/PageStack.h"
+#include "ipc/StoreMap.h"
 #include "SwapDir.h"
 
 class DiskIOStrategy;
@@ -192,7 +192,7 @@ Fs::Ufs::RebuildState::rebuildFromDirectory()
         return;
 
     const uint64_t expectedSize = sb.st_size > 0 ?
-        static_cast<uint64_t>(sb.st_size) : 0;
+                                  static_cast<uint64_t>(sb.st_size) : 0;
 
     StoreEntry tmpe;
     const bool parsed = storeRebuildParseEntry(buf, tmpe, key, counts,
@@ -160,28 +160,32 @@ Ipc::BaseMultiQueue::rateLimit(const int remoteProcessId) const
 }
 
 Ipc::OneToOneUniQueue &
-Ipc::BaseMultiQueue::inQueue(const int remoteProcessId) {
+Ipc::BaseMultiQueue::inQueue(const int remoteProcessId)
+{
     const OneToOneUniQueue &queue =
         const_cast<const BaseMultiQueue *>(this)->inQueue(remoteProcessId);
     return const_cast<OneToOneUniQueue &>(queue);
 }
 
 Ipc::OneToOneUniQueue &
-Ipc::BaseMultiQueue::outQueue(const int remoteProcessId) {
+Ipc::BaseMultiQueue::outQueue(const int remoteProcessId)
+{
     const OneToOneUniQueue &queue =
         const_cast<const BaseMultiQueue *>(this)->outQueue(remoteProcessId);
     return const_cast<OneToOneUniQueue &>(queue);
 }
 
 Ipc::QueueReader &
-Ipc::BaseMultiQueue::localReader() {
+Ipc::BaseMultiQueue::localReader()
+{
     const QueueReader &reader =
         const_cast<const BaseMultiQueue *>(this)->localReader();
     return const_cast<QueueReader &>(reader);
 }
 
 Ipc::QueueReader &
-Ipc::BaseMultiQueue::remoteReader(const int remoteProcessId) {
+Ipc::BaseMultiQueue::remoteReader(const int remoteProcessId)
+{
     const QueueReader &reader =
         const_cast<const BaseMultiQueue *>(this)->remoteReader(remoteProcessId);
     return const_cast<QueueReader &>(reader);
@@ -295,14 +299,14 @@ int
 Ipc::FewToFewBiQueue::remotesCount() const
 {
     return theLocalGroup == groupA ? metadata->theGroupBSize :
-        metadata->theGroupASize;
+           metadata->theGroupASize;
 }
 
 int
 Ipc::FewToFewBiQueue::remotesIdOffset() const
 {
     return theLocalGroup == groupA ? metadata->theGroupBIdOffset :
-        metadata->theGroupAIdOffset;
+           metadata->theGroupAIdOffset;
 }
 
 Ipc::FewToFewBiQueue::Metadata::Metadata(const int aGroupASize, const int aGroupAIdOffset, const int aGroupBSize, const int aGroupBIdOffset):
@@ -351,7 +355,7 @@ bool
 Ipc::MultiQueue::validProcessId(const int processId) const
 {
     return metadata->theProcessIdOffset <= processId &&
-        processId < metadata->theProcessIdOffset + metadata->theProcessCount;
+           processId < metadata->theProcessIdOffset + metadata->theProcessCount;
 }
 
 const Ipc::OneToOneUniQueue &
@@ -44,7 +44,7 @@ Ipc::StoreMap::compareVersions(const sfileno fileno, time_t newVersion) const
 
     if (const time_t diff = newVersion - inode.basics.timestamp)
         return diff < 0 ? -1 : +1;
- 
+
     return 0;
 }
 
@@ -196,7 +196,7 @@ Ipc::StoreMap::abortWriting(const sfileno fileno)
         s.waitingToBeFreed = true;
         s.lock.unlockExclusive();
         debugs(54, 5, "closed dirty entry " << fileno << " for writing " << path);
-	}
+    }
 }
 
 const Ipc::StoreMap::Anchor *
@@ -245,15 +245,15 @@ Ipc::StoreMap::freeEntryByKey(const cache_key *const key)
         if (s.sameKey(key))
             freeChain(idx, s, true);
         s.lock.unlockExclusive();
-	} else if (s.lock.lockShared()) {
+    } else if (s.lock.lockShared()) {
         if (s.sameKey(key))
             s.waitingToBeFreed = true; // mark to free it later
         s.lock.unlockShared();
     } else {
-       // we cannot be sure that the entry we found is ours because we do not
-       // have a lock on it, but we still check to minimize false deletions
-       if (s.sameKey(key))
-           s.waitingToBeFreed = true; // mark to free it later
+        // we cannot be sure that the entry we found is ours because we do not
+        // have a lock on it, but we still check to minimize false deletions
+        if (s.sameKey(key))
+            s.waitingToBeFreed = true; // mark to free it later
     }
 }
 
@@ -420,7 +420,6 @@ Ipc::StoreMap::anchorByKey(const cache_key *const key)
     return shared->slots[anchorIndexByKey(key)].anchor;
 }
 
-
 /* Ipc::StoreMapAnchor */
 
 Ipc::StoreMapAnchor::StoreMapAnchor(): start(0)
@@ -24,7 +24,6 @@ class StoreMapSlice
     Atomic::WordT<StoreMapSliceId> next; ///< ID of the next entry slice
 };
 
-
 /// Maintains shareable information about a StoreEntry as a whole.
 /// An anchor points to one or more StoreEntry slices. This is the
 /// only lockable part of shared StoreEntry information, providing
@@ -70,7 +69,7 @@ class StoreMapAnchor
     } basics;
 
     /// where the chain of StoreEntry slices begins [app]
-    Atomic::WordT<StoreMapSliceId> start; 
+    Atomic::WordT<StoreMapSliceId> start;
 
 #if 0
     /// possible persistent states
@@ -86,7 +85,8 @@ class StoreMapAnchor
 /// A hack to allocate one shared array for both anchors and slices.
 /// Anchors are indexed by store entry ID and are independent from each other.
 /// Slices are indexed by slice IDs and form entry chains using slice.next.
-class StoreMapSlot {
+class StoreMapSlot
+{
 public:
     StoreMapAnchor anchor; ///< information about store entry as a whole
     StoreMapSlice slice; ///< information about one stored entry piece
@@ -509,7 +509,8 @@ StoreEntry::lock(const char *context)
 }
 
 void
-StoreEntry::touch() {
+StoreEntry::touch()
+{
     lastref = squid_curtime;
     Store::Root().reference(*this);
 }
@@ -2026,9 +2027,9 @@ std::ostream &operator <<(std::ostream &os, const StoreEntry &e)
 
     // print only non-default status values, using unique letters
     if (e.mem_status != NOT_IN_MEMORY ||
-        e.store_status != STORE_PENDING ||
-        e.swap_status != SWAPOUT_NONE ||
-        e.ping_status != PING_NONE) {
+            e.store_status != STORE_PENDING ||
+            e.swap_status != SWAPOUT_NONE ||
+            e.ping_status != PING_NONE) {
         if (e.mem_status != NOT_IN_MEMORY) os << 'm';
         if (e.store_status != STORE_PENDING) os << 's';
         if (e.swap_status != SWAPOUT_NONE) os << 'w' << e.swap_status;
@@ -293,7 +293,7 @@ store_client::moreToSend() const
     // scheduleRead calls scheduleDiskRead which asserts on STORE_MEM_CLIENTs.
     const MemObject *mem = entry->mem_obj;
     return mem &&
-        mem->inmem_lo <= copyInto.offset && copyInto.offset < mem->endOffset();
+           mem->inmem_lo <= copyInto.offset && copyInto.offset < mem->endOffset();
 }
 
 static void
@@ -756,7 +756,7 @@ StoreController::get(const cache_key *key)
 {
     if (StoreEntry *e = find(key)) {
         // this is not very precise: some get()s are not initiated by clients
-        e->touch(); 
+        e->touch();
         return e;
     }
     return NULL;
@@ -941,7 +941,7 @@ int
 StoreController::transientReaders(const StoreEntry &e) const
 {
     return (transients && e.mem_obj && e.mem_obj->xitTable.index >= 0) ?
-        transients->readers(e) : 0;
+           transients->readers(e) : 0;
 }
 
 void
@@ -996,7 +996,7 @@ StoreController::allowCollapsing(StoreEntry *e, const RequestFlags &reqFlags,
     if (transients)
         transients->startWriting(e, reqFlags, reqMethod);
     debugs(20, 3, "may " << (transients && e->mem_obj->xitTable.index >= 0 ?
-           "SMP-" : "locally-") << "collapse " << *e);
+                             "SMP-" : "locally-") << "collapse " << *e);
 }
 
 void
@@ -162,10 +162,10 @@ doPages(StoreEntry *anEntry)
         // Quit if write() fails. Sio is going to call our callback, and that
         // will cleanup, but, depending on the fs, that call may be async.
         const bool ok = mem->swapout.sio->write(
-                     mem->data_hdr.NodeGet(page),
-                     swap_buf_len,
-                     -1,
-                     memNodeWriteComplete);
+                            mem->data_hdr.NodeGet(page),
+                            swap_buf_len,
+                            -1,
+                            memNodeWriteComplete);
 
         if (!ok || anEntry->swap_status != SWAPOUT_WRITING)
             return false;
@@ -32,8 +32,8 @@
 
 #include "squid.h"
 #include "MemBuf.h"
-#include "SwapDir.h"
 #include "store_rebuild.h"
+#include "SwapDir.h"
 #if HAVE_STRING_H
 #include <string.h>
 #endif