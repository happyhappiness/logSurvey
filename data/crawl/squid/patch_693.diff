@@ -479,6 +479,9 @@ diskerRead(IpcIoMsg &ipcIo)
 {
     debugs(0,0,HERE);
     const ssize_t read = pread(TheFile, ipcIo.buf, ipcIo.len, ipcIo.offset);
+    statCounter.syscalls.disk.reads++;
+    fd_bytes(TheFile, read, FD_READ);
+
     if (read >= 0) {
         ipcIo.xerrno = 0;
         const size_t len = static_cast<size_t>(read); // safe because read > 0
@@ -498,6 +501,9 @@ diskerWrite(IpcIoMsg &ipcIo)
 {
     debugs(0,0,HERE);
     const ssize_t wrote = pwrite(TheFile, ipcIo.buf, ipcIo.len, ipcIo.offset);
+    statCounter.syscalls.disk.writes++;
+    fd_bytes(TheFile, wrote, FD_WRITE);
+
     if (wrote >= 0) {
         ipcIo.xerrno = 0;
         const size_t len = static_cast<size_t>(wrote); // safe because wrote > 0
@@ -240,6 +240,22 @@ MemObject::size() const
     return object_sz;
 }
 
+int64_t
+MemObject::expectedReplySize() const {
+    debugs(20, 7, HERE << "object_sz: " << object_sz);
+    if (object_sz >= 0) // complete() has been called; we know the exact answer
+        return object_sz;
+
+    if (_reply) {
+        const int64_t clen = _reply->bodySize(method);
+        debugs(20, 7, HERE << "clen: " << clen);
+        if (clen >= 0 && _reply->hdr_sz > 0) // yuck: HttpMsg sets hdr_sz to 0
+            return clen + _reply->hdr_sz;
+    }
+
+    return -1; // not enough information to predict
+}
+
 void
 MemObject::reset()
 {
@@ -339,7 +355,7 @@ MemObject::trimSwappable()
      * there will be a chunk of the data which is not in memory
      * and is not yet on disk.
      * The -1 makes sure the page isn't freed until storeSwapOut has
-     * walked to the next page. (mem->swapout.memnode)
+     * walked to the next page.
      */
     int64_t on_disk;
 
@@ -64,6 +64,9 @@ class MemObject
     void replaceHttpReply(HttpReply *newrep);
     void stat (MemBuf * mb) const;
     int64_t endOffset () const;
+    /// negative if unknown; otherwise, expected object_sz, expected endOffset
+    /// maximum, and stored reply headers+body size (all three are the same)
+    int64_t expectedReplySize() const;
     int64_t size() const;
     void reset();
     int64_t lowestMemReaderOffset() const;
@@ -106,8 +109,7 @@ class MemObject
     {
 
     public:
-        int64_t queue_offset;     /* relative to in-mem data */
-        mem_node *memnode;      /* which node we're currently paging out */
+        int64_t queue_offset; ///< number of bytes sent to SwapDir for writing
         StoreIOState::Pointer sio;
     };
 
@@ -524,7 +524,7 @@ ServerStateData::maybePurgeOthers()
     purgeEntriesByHeader(request, reqUrl, theFinalReply, HDR_CONTENT_LOCATION);
 }
 
-// called (usually by kids) when we have final (possibly adapted) reply headers
+/// called when we have final (possibly adapted) reply headers; kids extend
 void
 ServerStateData::haveParsedReplyHeaders()
 {
@@ -112,7 +112,7 @@ class StoreEntry : public hash_link
     void purgeMem();
     void swapOut();
     bool swapOutAble() const;
-    void swapOutFileClose();
+    void swapOutFileClose(int how);
     const char *url() const;
     int checkCachable();
     int checkNegativeHit() const;
@@ -84,13 +84,19 @@ class StoreIOState : public RefCountable
 
     virtual void read_(char *buf, size_t size, off_t offset, STRCB * callback, void *callback_data) = 0;
     virtual void write(char const *buf, size_t size, off_t offset, FREE * free_func) = 0;
-    virtual void close() = 0;
+
+    typedef enum {
+       wroteAll, ///< success: caller supplied all data it wanted to swap out
+       writerGone, ///< failure: caller left before swapping out everything
+       readerDone ///< success or failure: either way, stop swapping in
+    } CloseHow;
+    virtual void close(int how) = 0; ///< finish or abort swapping per CloseHow
 
     sdirno swap_dirn;
     sfileno swap_filen;
     StoreEntry *e;		/* Need this so the FS layers can play god */
     mode_t mode;
-    off_t offset_;		/* current on-disk offset pointer */
+    off_t offset_; ///< number of bytes written or read for this entry so far
     STFNCB *file_callback;	/* called on delayed sfileno assignments */
     STIOCB *callback;
     void *callback_data;
@@ -107,7 +113,7 @@ class StoreIOState : public RefCountable
 
 StoreIOState::Pointer storeCreate(StoreEntry *, StoreIOState::STFNCB *, StoreIOState::STIOCB *, void *);
 StoreIOState::Pointer storeOpen(StoreEntry *, StoreIOState::STFNCB *, StoreIOState::STIOCB *, void *);
-SQUIDCEXTERN void storeClose(StoreIOState::Pointer);
+SQUIDCEXTERN void storeClose(StoreIOState::Pointer, int how);
 SQUIDCEXTERN void storeRead(StoreIOState::Pointer, char *, size_t, off_t, StoreIOState::STRCB *, void *);
 SQUIDCEXTERN void storeIOWrite(StoreIOState::Pointer, char const *, size_t, off_t, FREE *);
 
@@ -155,8 +155,6 @@ class StoreMeta
 /// \ingroup SwapStoreAPI
 SQUIDCEXTERN char *storeSwapMetaPack(tlv * tlv_list, int *length);
 /// \ingroup SwapStoreAPI
-SQUIDCEXTERN size_t storeSwapMetaSize(const StoreEntry * e);
-/// \ingroup SwapStoreAPI
 SQUIDCEXTERN tlv *storeSwapMetaBuild(StoreEntry * e);
 /// \ingroup SwapStoreAPI
 SQUIDCEXTERN void storeSwapTLVFree(tlv * n);
@@ -109,6 +109,30 @@ SwapDir::callback()
     return 0;
 }
 
+bool
+SwapDir::canStore(const StoreEntry &e, int64_t diskSpaceNeeded, int &load) const
+{
+    debugs(47,8, HERE << "cache_dir[" << index << "]: needs " <<
+           diskSpaceNeeded << " <? " << max_objsize);
+
+    if (EBIT_TEST(e.flags, ENTRY_SPECIAL))
+        return false; // we do not store Squid-generated entries
+
+    if (!objectSizeIsAcceptable(diskSpaceNeeded))
+        return false; // does not satisfy size limits
+
+    if (flags.read_only)
+        return false; // cannot write at all
+
+    if (cur_size > max_size)
+        return false; // already overflowing
+
+    /* Return 999 (99.9%) constant load; TODO: add a named constant for this */
+    load = 999;
+    return true; // kids may provide more tests and should report true load
+}
+
+
 void
 SwapDir::sync() {}
 
@@ -176,8 +176,8 @@ class SwapDir : public Store
     virtual bool doubleCheck(StoreEntry &);	/* Double check the obj integrity */
     virtual void statfs(StoreEntry &) const;	/* Dump fs statistics */
     virtual void maintain();	/* Replacement maintainence */
-    /* <0 == error. > 1000 == error */
-    virtual int canStore(StoreEntry const &)const = 0; /* Check if the fs will store an object */
+    /// check whether we can store the entry; if we can, report current load
+    virtual bool canStore(const StoreEntry &e, int64_t diskSpaceNeeded, int &load) const = 0;
     /* These two are notifications */
     virtual void reference(StoreEntry &);	/* Reference this object */
     virtual void dereference(StoreEntry &);	/* Unreference this object */
@@ -1036,6 +1036,8 @@ clientReplyContext::checkTransferDone()
 int
 clientReplyContext::storeOKTransferDone() const
 {
+    assert(http->storeEntry()->objectLen() >= 0);
+    assert(http->storeEntry()->objectLen() >= headers_sz);
     if (http->out.offset >= http->storeEntry()->objectLen() - headers_sz) {
         debugs(88,3,HERE << "storeOKTransferDone " <<
                " out.offset=" << http->out.offset <<
@@ -37,7 +37,7 @@ class CossSwapDir : public SwapDir, public IORequestor
     virtual StoreSearch *search(String const url, HttpRequest *);
     virtual void unlink (StoreEntry &);
     virtual void statfs (StoreEntry &)const;
-    virtual int canStore(StoreEntry const &)const;
+    virtual bool canStore(const StoreEntry &e, int64_t diskSpaceNeeded, int &load) const;
     virtual int callback();
     virtual void sync();
     virtual StoreIOState::Pointer createStoreIO(StoreEntry &, StoreIOState::STFNCB *, StoreIOState::STIOCB *, void *);
@@ -960,26 +960,14 @@ CossSwapDir::~CossSwapDir()
     safe_free(stripe_path);
 }
 
-/*
- * storeCossDirCheckObj
- *
- * This routine is called by storeDirSelectSwapDir to see if the given
- * object is able to be stored on this filesystem. COSS filesystems will
- * not store everything. We don't check for maxobjsize here since its
- * done by the upper layers.
- */
-int
-CossSwapDir::canStore(StoreEntry const &e)const
+bool
+CossSwapDir::canStore(const StoreEntry &e, int64_t diskSpaceNeeded, int &load) const
 {
+    if (!SwapDir::canStore(e, diskSpaceNeeded, load))
+        return false;
 
-    /* Check if the object is a special object, we can't cache these */
-
-    if (EBIT_TEST(e.flags, ENTRY_SPECIAL))
-        return -1;
-
-    /* Otherwise, we're ok */
-    /* Return load, cs->aq.aq_numpending out of MAX_ASYNCOP */
-    return io->load();
+    load = io->load();
+    return true;
 }
 
 /*
@@ -31,7 +31,7 @@ Rock::DirMap::DirMap(const char *const aPath):
            entryLimit());
 }
 
-StoreEntryBasics *
+Rock::StoreEntryBasics *
 Rock::DirMap::openForWriting(const cache_key *const key, sfileno &fileno)
 {
     debugs(79, 5, HERE << " trying to open slot for key " << storeKeyText(key)
@@ -95,8 +95,26 @@ Rock::DirMap::abortIo(const sfileno fileno)
         closeForReading(fileno);
 }
 
+const Rock::StoreEntryBasics *
+Rock::DirMap::peekAtReader(const sfileno fileno) const
+{
+    assert(valid(fileno));
+    const Slot &s = shared->slots[fileno];
+    switch (s.state) {
+    case Slot::Readable:
+        return &s.seBasics; // immediate access by lock holder so no locking
+    case Slot::Writeable:
+        return NULL; // cannot read the slot when it is being written
+    case Slot::Empty:
+        assert(false); // must be locked for reading or writing
+    }
+    assert(false); // not reachable
+    return NULL;
+}
+
 bool
-Rock::DirMap::putAt(const StoreEntry &e, const sfileno fileno)
+Rock::DirMap::putAt(const DbCellHeader &header, const StoreEntry &e,
+                    const sfileno fileno)
 {
     const cache_key *key = static_cast<const cache_key*>(e.key);
     debugs(79, 5, HERE << " trying to open slot for key " << storeKeyText(key)
@@ -120,7 +138,7 @@ Rock::DirMap::putAt(const StoreEntry &e, const sfileno fileno)
         if (s.state == Slot::Empty) // we may also overwrite a Readable slot
             ++shared->count;
         s.setKey(static_cast<const cache_key*>(e.key));
-        s.seBasics.set(e);
+        s.seBasics.set(header, e);
         s.state = Slot::Readable;
         s.releaseExclusiveLock();
         debugs(79, 5, HERE << " put slot at " << fileno << " for key " <<
@@ -145,7 +163,7 @@ Rock::DirMap::free(const sfileno fileno)
     freeIfNeeded(s);
 }
 
-const StoreEntryBasics *
+const Rock::StoreEntryBasics *
 Rock::DirMap::openForReading(const cache_key *const key, sfileno &fileno)
 {
     debugs(79, 5, HERE << " trying to open slot for key " << storeKeyText(key)
@@ -168,7 +186,7 @@ Rock::DirMap::openForReading(const cache_key *const key, sfileno &fileno)
     return 0;
 }
 
-const StoreEntryBasics *
+const Rock::StoreEntryBasics *
 Rock::DirMap::openForReadingAt(const sfileno fileno)
 {
     debugs(79, 5, HERE << " trying to open slot at " << fileno << " for "
@@ -216,6 +234,14 @@ Rock::DirMap::full() const
     return entryCount() >= entryLimit();
 }
 
+void
+Rock::DirMap::updateStats(MapStats &stats) const
+{
+    stats.capacity += shared->limit;
+    for (int i = 0; i < shared->limit; ++i)
+        shared->slots[i].updateStats(stats);
+}
+
 bool
 Rock::DirMap::valid(const int pos) const
 {
@@ -333,14 +359,38 @@ Rock::Slot::switchExclusiveToSharedLock()
     releaseExclusiveLock();
 }
 
+void
+Rock::Slot::updateStats(MapStats &stats) const
+{
+    switch (state) {
+    case Readable:
+        ++stats.readable;
+        stats.readers += readers;
+        break;
+    case Writeable:
+        ++stats.writeable;
+        stats.writers += writers;
+        break;
+    case Empty:
+        ++stats.empty;
+        break;
+    }
+
+    if (waitingToBeFreed)
+        ++stats.marked;
+}
+
+
 Rock::DirMap::Shared::Shared(const int aLimit): limit(aLimit), count(0)
 {
 }
 
 void
-StoreEntryBasics::set(const StoreEntry &from)
+Rock::StoreEntryBasics::set(const DbCellHeader &aHeader, const StoreEntry &from)
 {
+    assert(from.swap_file_sz > 0);
     memset(this, 0, sizeof(*this));
+    header = aHeader;
     timestamp = from.timestamp;
     lastref = from.lastref;
     expires = from.expires;
@@ -349,3 +399,40 @@ StoreEntryBasics::set(const StoreEntry &from)
     refcount = from.refcount;
     flags = from.flags;
 }
+
+
+/* MapStats */
+
+Rock::MapStats::MapStats()
+{
+    memset(this, 0, sizeof(*this));
+}
+ 
+void
+Rock::MapStats::dump(StoreEntry &e) const
+{
+    storeAppendPrintf(&e, "Available slots: %9d\n", capacity);
+
+    if (!capacity)
+        return;
+
+    storeAppendPrintf(&e, "Readable slots:  %9d %6.2f%%\n",
+        readable, (100.0 * readable / capacity));
+    storeAppendPrintf(&e, "Filling slots:   %9d %6.2f%%\n",
+        writeable, (100.0 * writeable / capacity));
+    storeAppendPrintf(&e, "Empty slots:     %9d %6.2f%%\n",
+        empty, (100.0 * empty / capacity));
+
+    if (readers || writers) {
+        const int locks = readers + writers;
+        storeAppendPrintf(&e, "Readers:         %9d %6.2f%%\n",
+            readers, (100.0 * readers / locks));
+        storeAppendPrintf(&e, "Writers:         %9d %6.2f%%\n",
+            writers, (100.0 * writers / locks));
+    }
+
+    if (readable + writeable) {
+        storeAppendPrintf(&e, "Marked slots:    %9d %6.2f%%\n",
+            marked, (100.0 * marked / (readable + writeable)));
+    }
+}
@@ -1,12 +1,17 @@
 #ifndef SQUID_FS_ROCK_DIR_MAP_H
 #define SQUID_FS_ROCK_DIR_MAP_H
 
+#include "fs/rock/RockFile.h"
 #include "ipc/AtomicWord.h"
 #include "ipc/SharedMemory.h"
 
+namespace Rock {
+
 class StoreEntryBasics {
 public:
-    void set(const StoreEntry &from);
+    void set(const DbCellHeader &aHeader, const StoreEntry &anEntry);
+
+    DbCellHeader header; ///< rock-specific entry metadata
 
     /* START OF ON-DISK STORE_META_STD TLV field */
     time_t timestamp;
@@ -19,7 +24,21 @@ class StoreEntryBasics {
     /* END OF ON-DISK STORE_META_STD */
 };
 
-namespace Rock {
+/// aggregates basic map performance measurements; all numbers are approximate
+class MapStats {
+public:
+    MapStats();
+
+    void dump(StoreEntry &e) const;
+
+    int capacity; ///< the total number of slots in the map
+    int readable; ///< number of slots in Readable state
+    int writeable; ///< number of slots in Writeable state
+    int empty; ///< number of slots in Empty state
+    int readers; ///< sum of slot.readers
+    int writers; ///< sum of slot.writers
+    int marked; ///< number of slots marked for freeing
+};
 
 /// DirMap entry
 class Slot {
@@ -40,6 +59,9 @@ class Slot {
     void releaseExclusiveLock(); ///< undo successful exclusiveLock()
     void switchExclusiveToSharedLock(); ///< trade exclusive for shared access
 
+    /// adds approximate current stats to the supplied ones
+    void updateStats(MapStats &stats) const;
+
 public:
     // we want two uint64_t, but older GCCs lack __sync_fetch_and_add_8
     AtomicWordT<uint32_t> key_[4]; ///< MD5 entry key
@@ -66,7 +88,10 @@ class DirMap
     void closeForWriting(const sfileno fileno);
 
     /// stores entry info at the requested slot or returns false
-    bool putAt(const StoreEntry &e, const sfileno fileno);
+    bool putAt(const DbCellHeader &header, const StoreEntry &e, const sfileno fileno);
+
+    /// only works on locked entries; returns nil unless the slot is readable
+    const StoreEntryBasics *peekAtReader(const sfileno fileno) const;
 
     /// mark the slot as waiting to be freed and, if possible, free it
     void free(const sfileno fileno);
@@ -86,6 +111,9 @@ class DirMap
     int entryCount() const; ///< number of used slots
     int entryLimit() const; ///< maximum number of slots that can be used
 
+    /// adds approximate current stats to the supplied ones
+    void updateStats(MapStats &stats) const;
+
     static int AbsoluteEntryLimit(); ///< maximum entryLimit() possible
 
 private:
@@ -0,0 +1,24 @@
+#ifndef SQUID_FS_ROCK_DB_CELL_H
+#define SQUID_FS_ROCK_DB_CELL_H
+
+// XXX: rename to fs/rock/RockDbCell.{cc,h}
+
+namespace Rock {
+
+/// \ingroup Rock
+/// meta-information at the beginning of every db cell
+class DbCellHeader
+{
+public:
+    DbCellHeader(): payloadSize(0), reserved(0) {}
+
+    /// whether the freshly loaded header fields make sense
+    bool sane() const { return payloadSize >= 0 && reserved == 0; }
+
+    int64_t payloadSize; ///< cell contents size excluding this header
+    int64_t reserved; ///< reserved for future use (next cell pointer?)
+};
+
+} // namespace Rock
+
+#endif /* SQUID_FS_ROCK_DB_CELL_H */
@@ -4,6 +4,8 @@
  * DEBUG: section 79    Disk IO Routines
  */
 
+#include "config.h"
+#include "MemObject.h"
 #include "Parsing.h"
 #include "DiskIO/DiskIOModule.h"
 #include "DiskIO/DiskIOStrategy.h"
@@ -18,11 +20,11 @@ Rock::IoState::IoState(SwapDir *dir,
     StoreIOState::STIOCB *cbIo,
     void *data):
     slotSize(0),
-    entrySize(0)
+    diskOffset(-1),
+    payloadEnd(-1)
 {
     e = anEntry;
-    swap_filen = e->swap_filen;
-    swap_dirn = dir->index;
+    // swap_filen, swap_dirn, diskOffset, and payloadEnd are set by the caller
     slotSize = dir->max_objsize;
     file_callback = cbFile;
     callback = cbIo;
@@ -48,45 +50,55 @@ Rock::IoState::file(const RefCount<DiskFile> &aFile)
 }
 
 void
-Rock::IoState::read_(char *buf, size_t len, off_t off, STRCB *cb, void *data)
+Rock::IoState::read_(char *buf, size_t len, off_t coreOff, STRCB *cb, void *data)
 {
     assert(theFile != NULL);
     assert(theFile->canRead());
+    assert(coreOff >= 0);
+    offset_ = coreOff;
+
+    // we skip our cell header; it is only read when building the map
+    const int64_t cellOffset = sizeof(DbCellHeader) +
+        static_cast<int64_t>(coreOff);
+    assert(cellOffset <= payloadEnd);
 
     // Core specifies buffer length, but we must not exceed stored entry size
-    assert(off >= 0);
-    assert(entrySize >= 0);
-    const int64_t offset = static_cast<int64_t>(off);
-    assert(offset <= entrySize);
-    if (offset + (int64_t)len > entrySize)
-        len = entrySize - offset;
+    if (cellOffset + (int64_t)len > payloadEnd)
+        len = payloadEnd - cellOffset;
 
     assert(read.callback == NULL);
     assert(read.callback_data == NULL);
     read.callback = cb;
     read.callback_data = cbdataReference(data);
 
-    theFile->read(new ReadRequest(::ReadRequest(buf, offset_ + offset, len), this));
+    theFile->read(new ReadRequest(
+                  ::ReadRequest(buf, diskOffset + cellOffset, len), this));
 }
 
 // We only buffer data here; we actually write when close() is called.
 // We buffer, in part, to avoid forcing OS to _read_ old unwritten portions
 // of the slot when the write does not end at the page or sector boundary.
 void
-Rock::IoState::write(char const *buf, size_t size, off_t offset, FREE *dtor)
+Rock::IoState::write(char const *buf, size_t size, off_t coreOff, FREE *dtor)
 {
     // TODO: move to create?
-    if (!offset) {
+    if (!coreOff) {
         assert(theBuf.isNull());
-        assert(entrySize >= 0);
-        theBuf.init(min(entrySize, slotSize), slotSize);
+        assert(payloadEnd <= slotSize);
+        theBuf.init(min(payloadEnd, slotSize), slotSize);
+        // start with our header; TODO: consider making it a trailer
+        DbCellHeader header;
+        assert(static_cast<int64_t>(sizeof(header)) <= payloadEnd);
+        header.payloadSize = payloadEnd - sizeof(header);
+        theBuf.append(reinterpret_cast<const char*>(&header), sizeof(header));
     } else {
         // Core uses -1 offset as "append". Sigh.
-        assert(offset == -1);
+        assert(coreOff == -1);
         assert(!theBuf.isNull());
     }
 
     theBuf.append(buf, size);
+    offset_ += size; // so that Core thinks we wrote it
 
     if (dtor)
         (dtor)(const_cast<char*>(buf)); // cast due to a broken API?
@@ -103,30 +115,32 @@ Rock::IoState::startWriting()
     // TODO: if DiskIO module is mmap-based, we should be writing whole pages
     // to avoid triggering read-page;new_head+old_tail;write-page overheads
 
-    debugs(79, 5, HERE << swap_filen << " at " << offset_ << '+' <<
+    debugs(79, 5, HERE << swap_filen << " at " << diskOffset << '+' <<
         theBuf.contentSize());
 
     assert(theBuf.contentSize() <= slotSize);
     // theFile->write may call writeCompleted immediatelly
-    theFile->write(new WriteRequest(::WriteRequest(theBuf.content(), offset_,
-        theBuf.contentSize(), theBuf.freeFunc()), this));
+    theFile->write(new WriteRequest(::WriteRequest(theBuf.content(),
+                   diskOffset, theBuf.contentSize(), theBuf.freeFunc()), this));
 }
 
 // 
 void
 Rock::IoState::finishedWriting(const int errFlag)
 {
+    // we incremented offset_ while accumulating data in write()
     callBack(errFlag);
 }
 
 void
-Rock::IoState::close()
+Rock::IoState::close(int how)
 {
-    debugs(79, 3, HERE << swap_filen << " at " << offset_);
-    if (!theBuf.isNull())
+    debugs(79, 3, HERE << swap_filen << " accumulated: " << offset_ <<
+        " how=" << how);
+    if (how == wroteAll && !theBuf.isNull())
         startWriting();
     else
-        callBack(0);
+        callBack(how == writerGone ? DISK_ERROR : 0); // TODO: add DISK_CALLER_GONE
 }
 
 /// close callback (STIOCB) dialer: breaks dependencies and 
@@ -24,13 +24,17 @@ class IoState: public ::StoreIOState
     // ::StoreIOState API
     virtual void read_(char *buf, size_t size, off_t offset, STRCB * callback, void *callback_data);
     virtual void write(char const *buf, size_t size, off_t offset, FREE * free_func);
-    virtual void close();
+    virtual void close(int how);
 
     /// called by SwapDir when writing is done
     void finishedWriting(int errFlag);
 
     int64_t slotSize; ///< db cell size
-    int64_t entrySize; ///< planned or actual stored size for the entry
+    int64_t diskOffset; ///< the start of this cell inside the db file
+
+    /// when reading: number of bytes previously written to the db cell;
+    /// when writing: maximum payload offset in a db cell
+    int64_t payloadEnd;
 
     MEMPROXY_CLASS(IoState);
 
@@ -4,8 +4,10 @@
  * DEBUG: section 79    Disk IO Routines
  */
 
+#include "config.h"
 #include "fs/rock/RockRebuild.h"
 #include "fs/rock/RockSwapDir.h"
+#include "fs/rock/RockFile.h"
 
 CBDATA_NAMESPACED_CLASS_INIT(Rock, Rebuild);
 
@@ -104,12 +106,38 @@ Rock::Rebuild::doOneEntry() {
     debugs(47,5, HERE << sd->index << " fileno " << fileno << " at " <<
         dbOffset << " <= " << dbSize);
 
+    ++counts.scancount;
+
     if (lseek(fd, dbOffset, SEEK_SET) < 0)
         failure("cannot seek to db entry", errno);
 
+    MemBuf buf;
+    buf.init(SM_PAGE_SIZE, SM_PAGE_SIZE);
+
+    if (!storeRebuildLoadEntry(fd, sd->index, buf, counts))
+        return;
+
+    // get our header
+    DbCellHeader header;
+    if (buf.contentSize() < static_cast<mb_size_t>(sizeof(header))) {
+        debugs(47, 1, "cache_dir[" << sd->index << "]: " <<
+            "truncated swap entry meta data at " << dbOffset);
+        counts.invalid++;
+        return;
+    }
+    memcpy(&header, buf.content(), sizeof(header));
+
+    if (!header.sane()) {
+        debugs(47, 1, "cache_dir[" << sd->index << "]: " <<
+            "malformed rock db cell header at " << dbOffset);
+        counts.invalid++;
+        return;
+    }
+    buf.consume(sizeof(header)); // optimize to avoid memmove()
+
     cache_key key[SQUID_MD5_DIGEST_LENGTH];
     StoreEntry loadedE;
-    if (!storeRebuildLoadEntry(fd, loadedE, key, counts, 0)) {
+    if (!storeRebuildParseEntry(buf, loadedE, key, counts, header.payloadSize)) {
         // skip empty slots
         if (loadedE.swap_filen > 0 || loadedE.swap_file_sz > 0) {
             counts.invalid++;
@@ -125,7 +153,7 @@ Rock::Rebuild::doOneEntry() {
     counts.objcount++;
     // loadedE->dump(5);
 
-    sd->addEntry(fileno, loadedE);
+    sd->addEntry(fileno, header, loadedE);
 }
 
 void
@@ -296,13 +296,13 @@ Rock::SwapDir::rebuild() {
 /* Add a new object to the cache with empty memory copy and pointer to disk
  * use to rebuild store from disk. Based on UFSSwapDir::addDiskRestore */
 bool
-Rock::SwapDir::addEntry(const int fileno, const StoreEntry &from)
+Rock::SwapDir::addEntry(const int fileno, const DbCellHeader &header, const StoreEntry &from)
 {
     debugs(47, 8, HERE << &from << ' ' << from.getMD5Text() <<
        ", fileno="<< std::setfill('0') << std::hex << std::uppercase <<
        std::setw(8) << fileno);
 
-    if (map->putAt(from, fileno)) {
+    if (map->putAt(header, from, fileno)) {
         // we do not add this entry to store_table so core will not updateSize
         updateSize(from.swap_file_sz, +1);
         return true;
@@ -312,24 +312,23 @@ Rock::SwapDir::addEntry(const int fileno, const StoreEntry &from)
 }
 
 
-int
-Rock::SwapDir::canStore(const StoreEntry &e) const
+bool
+Rock::SwapDir::canStore(const StoreEntry &e, int64_t diskSpaceNeeded, int &load) const
 {
-    debugs(47,8, HERE << e.swap_file_sz << " ? " << max_objsize);
-
-    if (EBIT_TEST(e.flags, ENTRY_SPECIAL))
-        return -1;
+    if (!::SwapDir::canStore(e, sizeof(DbCellHeader)+diskSpaceNeeded, load))
+        return false;
 
-    if (!theFile || !theFile->canRead() || !theFile->canWrite())
-        return -1;
+    if (!theFile || !theFile->canWrite())
+        return false;
 
     if (!map)
-        return -1;
+        return false;
 
     if (io->shedLoad())
-        return -1;
+        return false;
 
-    return io->load();
+    load = io->load();
+    return true;
 }
 
 StoreIOState::Pointer
@@ -340,32 +339,41 @@ Rock::SwapDir::createStoreIO(StoreEntry &e, StoreIOState::STFNCB *cbFile, StoreI
         return NULL;
     }
 
+    // compute payload size for our cell header, using StoreEntry info
+    // careful: e.objectLen() may still be negative here
+    const int64_t expectedReplySize = e.mem_obj->expectedReplySize();
+    assert(expectedReplySize >= 0); // must know to prevent cell overflows
+    assert(e.mem_obj->swap_hdr_sz > 0);
+    DbCellHeader header;
+    header.payloadSize = e.mem_obj->swap_hdr_sz + expectedReplySize;
+    const int64_t payloadEnd = sizeof(DbCellHeader) + header.payloadSize;
+    assert(payloadEnd <= max_objsize);
+
     sfileno fileno;
     StoreEntryBasics *const basics =
         map->openForWriting(reinterpret_cast<const cache_key *>(e.key), fileno);
     if (!basics) {
         debugs(47, 5, HERE << "Rock::SwapDir::createStoreIO: map->add failed");
         return NULL;
     }
-    basics->set(e);
+    e.swap_file_sz = header.payloadSize; // and will be copied to the map
+    basics->set(header, e);
 
-    // XXX: We rely on our caller, storeSwapOutStart(), to set e->fileno.
+    // XXX: We rely on our caller, storeSwapOutStart(), to set e.fileno.
     // If that does not happen, the entry will not decrement the read level!
 
     IoState *sio = new IoState(this, &e, cbFile, cbIo, data);
 
     sio->swap_dirn = index;
     sio->swap_filen = fileno;
-    sio->offset_ = diskOffset(sio->swap_filen);
-    sio->entrySize = e.objectLen() + e.mem_obj->swap_hdr_sz;
+    sio->payloadEnd = payloadEnd;
+    sio->diskOffset = diskOffset(sio->swap_filen);
 
     debugs(47,5, HERE << "dir " << index << " created new fileno " <<
         std::setfill('0') << std::hex << std::uppercase << std::setw(8) <<
-        sio->swap_filen << std::dec << " at " << sio->offset_ << " size: " <<
-        sio->entrySize << " (" << e.objectLen() << '+' <<
-        e.mem_obj->swap_hdr_sz << ")");
+        sio->swap_filen << std::dec << " at " << sio->diskOffset);
 
-    assert(sio->offset_ + sio->entrySize <= diskOffsetLimit());
+    assert(sio->diskOffset + payloadEnd <= diskOffsetLimit());
 
     sio->file(theFile);
 
@@ -376,15 +384,18 @@ Rock::SwapDir::createStoreIO(StoreEntry &e, StoreIOState::STFNCB *cbFile, StoreI
 int64_t
 Rock::SwapDir::diskOffset(int filen) const
 {
+    assert(filen >= 0);
     return HeaderSize + max_objsize*filen;
 }
 
 int64_t
 Rock::SwapDir::diskOffsetLimit() const
 {
+    assert(map);
     return diskOffset(map->entryLimit());
 }
 
+// tries to open an old or being-written-to entry with swap_filen for reading
 StoreIOState::Pointer
 Rock::SwapDir::openStoreIO(StoreEntry &e, StoreIOState::STFNCB *cbFile, StoreIOState::STIOCB *cbIo, void *data)
 {
@@ -398,23 +409,29 @@ Rock::SwapDir::openStoreIO(StoreEntry &e, StoreIOState::STFNCB *cbFile, StoreIOS
         return NULL;
     }
 
-    // The only way the entry has swap_filen is if get() locked it for reading
-    // so we do not need to map->openForReadingAt(swap_filen) again here.
+    // The are two ways an entry can get swap_filen: our get() locked it for
+    // reading or our storeSwapOutStart() locked it for writing. Peeking at our
+    // locked entry is safe, but no support for reading a filling entry.
+    const StoreEntryBasics *basics = map->peekAtReader(e.swap_filen);
+    if (!basics)
+        return NULL; // we were writing afterall
 
     IoState *sio = new IoState(this, &e, cbFile, cbIo, data);
 
     sio->swap_dirn = index;
     sio->swap_filen = e.swap_filen;
+    sio->payloadEnd = sizeof(DbCellHeader) + basics->header.payloadSize;
+    assert(sio->payloadEnd <= max_objsize); // the payload fits the slot
+
     debugs(47,5, HERE << "dir " << index << " has old fileno: " <<
         std::setfill('0') << std::hex << std::uppercase << std::setw(8) <<
         sio->swap_filen);
 
-    assert(map->valid(sio->swap_filen));
-    sio->offset_ = diskOffset(sio->swap_filen);
-    sio->entrySize = e.swap_file_sz;
-    assert(sio->entrySize <= max_objsize);
+    assert(basics->swap_file_sz > 0);
+    assert(basics->swap_file_sz == e.swap_file_sz);
 
-    assert(sio->offset_ + sio->entrySize <= diskOffsetLimit());
+    sio->diskOffset = diskOffset(sio->swap_filen);
+    assert(sio->diskOffset + sio->payloadEnd <= diskOffsetLimit());
 
     sio->file(theFile);
     return sio;
@@ -460,7 +477,9 @@ Rock::SwapDir::readCompleted(const char *buf, int rlen, int errflag, RefCount< :
     assert(request);
     IoState::Pointer sio = request->sio;
 
-    // do not increment sio->offset_: callers always supply relative offset
+    if (errflag == DISK_OK && rlen > 0)
+        sio->offset_ += rlen;
+    assert(sio->diskOffset + sio->offset_ <= diskOffsetLimit()); // post-factum
 
     StoreIOState::STRCB *callback = sio->read.callback;
     assert(callback);
@@ -481,7 +500,7 @@ Rock::SwapDir::writeCompleted(int errflag, size_t rlen, RefCount< ::WriteRequest
     if (errflag == DISK_OK) {
         // close, assuming we only write once; the entry gets the read lock
         map->closeForWriting(sio.swap_filen);
-        // and sio.offset_ += rlen;
+        // do not increment sio.offset_ because we do it in sio->write()
     } else {
         // Do not abortWriting here. The entry should keep the write lock
         // instead of losing association with the store and confusing core.
@@ -490,15 +509,15 @@ Rock::SwapDir::writeCompleted(int errflag, size_t rlen, RefCount< ::WriteRequest
 
     // TODO: always compute cur_size based on map, do not store it
     cur_size = (HeaderSize + max_objsize * map->entryCount()) >> 10;
-    assert(sio.offset_ <= diskOffsetLimit()); // post-factum check
+    assert(sio.diskOffset + sio.offset_ <= diskOffsetLimit()); // post-factum
 
     sio.finishedWriting(errflag);
 }
 
 bool
 Rock::SwapDir::full() const
 {
-    return map->full();
+    return map && map->full();
 }
 
 void
@@ -625,9 +644,17 @@ Rock::SwapDir::statfs(StoreEntry &e) const
     if (map) {
         const int limit = map->entryLimit();
         storeAppendPrintf(&e, "Maximum entries: %9d\n", limit);
-        if (limit > 0)
+        if (limit > 0) {
+            const int entryCount = map->entryCount();
             storeAppendPrintf(&e, "Current entries: %9d %.2f%%\n",
-                map->entryCount(), (100.0 * map->entryCount() / limit));
+                entryCount, (100.0 * entryCount / limit));
+
+            if (limit < 100) { // XXX: otherwise too expensive to count
+                MapStats stats;
+                map->updateStats(stats);
+                stats.dump(e);
+            }
+        }
     }    
 
     storeAppendPrintf(&e, "Pending operations: %d out of %d\n",
@@ -32,7 +32,7 @@ class SwapDir: public ::SwapDir, public IORequestor
     virtual bool needsDiskStrand() const;
     virtual void create();
     virtual void init();
-    virtual int canStore(StoreEntry const &) const;
+    virtual bool canStore(const StoreEntry &e, int64_t diskSpaceNeeded, int &load) const;
     virtual StoreIOState::Pointer createStoreIO(StoreEntry &, StoreIOState::STFNCB *, StoreIOState::STIOCB *, void *);
     virtual StoreIOState::Pointer openStoreIO(StoreEntry &, StoreIOState::STFNCB *, StoreIOState::STIOCB *, void *);
     virtual void maintain();
@@ -55,7 +55,7 @@ class SwapDir: public ::SwapDir, public IORequestor
 
     void rebuild(); ///< starts loading and validating stored entry metadata
     ///< used to add entries successfully loaded during rebuild
-    bool addEntry(const int fileno, const StoreEntry &from);
+    bool addEntry(const int fileno, const DbCellHeader &header, const StoreEntry &from);
 
     bool full() const; ///< no more entries can be stored without purging
     void trackReferences(StoreEntry &e); ///< add to replacement policy scope
@@ -57,13 +57,17 @@ int *UFSSwapDir::UFSDirToGlobalDirMapping = NULL;
  * object is able to be stored on this filesystem. UFS filesystems will
  * happily store anything as long as the LRU time isn't too small.
  */
-int
-UFSSwapDir::canStore(StoreEntry const &e)const
+bool
+UFSSwapDir::canStore(const StoreEntry &e, int64_t diskSpaceNeeded, int &load) const
 {
+    if (!SwapDir::canStore(e, diskSpaceNeeded, load))
+        return false;
+
     if (IO->shedLoad())
-        return -1;
+        return false;
 
-    return IO->load();
+    load = IO->load();
+    return true;
 }
 
 
@@ -190,11 +190,11 @@ UFSStoreState::closeCompleted()
  * when it is safe to actually signal the lower layer for closing.
  */
 void
-UFSStoreState::close()
+UFSStoreState::close(int)
 {
     debugs(79, 3, "UFSStoreState::close: dirno " << swap_dirn  << ", fileno "<<
            std::setfill('0') << std::hex << std::uppercase << std::setw(8) << swap_filen);
-    tryClosing();
+    tryClosing(); // UFS does not distinguish different closure types
 }
 
 void
@@ -398,8 +398,13 @@ RebuildState::rebuildFromDirectory()
             continue;
         }
 
+        MemBuf buf;
+        buf.init(SM_PAGE_SIZE, SM_PAGE_SIZE);
+        if (!storeRebuildLoadEntry(fd, sd->index, buf, counts))
+            return;
+
         StoreEntry tmpe;
-        const bool loaded = storeRebuildLoadEntry(fd, tmpe, key, counts,
+        const bool loaded = storeRebuildParseEntry(buf, tmpe, key, counts,
             (int64_t)sb.st_size);
 
         file_close(fd);
@@ -62,7 +62,7 @@ class UFSSwapDir : public SwapDir
     virtual void unlink(StoreEntry &);
     virtual void statfs(StoreEntry &)const;
     virtual void maintain();
-    virtual int canStore(StoreEntry const &)const;
+    virtual bool canStore(const StoreEntry &e, int64_t diskSpaceNeeded, int &load) const;
     virtual void reference(StoreEntry &);
     virtual void dereference(StoreEntry &);
     virtual StoreIOState::Pointer createStoreIO(StoreEntry &, StoreIOState::STFNCB *, StoreIOState::STIOCB *, void *);
@@ -206,7 +206,7 @@ class UFSStoreState : public StoreIOState, public IORequestor
     void operator delete (void *);
     UFSStoreState(SwapDir * SD, StoreEntry * anEntry, STIOCB * callback_, void *callback_data_);
     ~UFSStoreState();
-    virtual void close();
+    virtual void close(int how);
     virtual void closeCompleted();
     // protected:
     virtual void ioCompletedNotification();
@@ -139,8 +139,8 @@ Mgr::Forwarder::handleRemoteAck()
     Must(entry != NULL);
 
     requestId = 0;
-    EBIT_CLR(entry->flags, ENTRY_FWD_HDR_WAIT);
-    entry->complete();
+    // Do not clear ENTRY_FWD_HDR_WAIT or do entry->complete() because
+    // it will trigger our client side processing. Let job cleanup close.
 }
 
 /// Mgr::Forwarder::requestTimedOut wrapper
@@ -1510,9 +1510,8 @@ peerCountMcastPeersDone(void *data)
 
     cbdataReferenceDone(psstate->callback_data);
 
-    EBIT_SET(fake->flags, ENTRY_ABORTED);
+    fake->abort(); // sets ENTRY_ABORTED and initiates releated cleanup
     HTTPMSGUNLOCK(fake->mem_obj->request);
-    fake->releaseRequest();
     fake->unlock();
     HTTPMSGUNLOCK(psstate->request);
     cbdataFree(psstate);
@@ -535,10 +535,12 @@ SQUIDCEXTERN void storeRebuildStart(void);
 SQUIDCEXTERN void storeRebuildComplete(struct _store_rebuild_data *);
 SQUIDCEXTERN void storeRebuildProgress(int sd_index, int total, int sofar);
 
-/// tries to load and validate entry metadata; returns tmp entry on success
-SQUIDCEXTERN bool storeRebuildLoadEntry(int fd, StoreEntry &e, cache_key *key, struct _store_rebuild_data &counts, uint64_t expectedSize);
+/// loads entry from disk; fills supplied memory buffer on success
+bool storeRebuildLoadEntry(int fd, int diskIndex, MemBuf &buf, struct _store_rebuild_data &counts);
+/// parses entry buffer and validates entry metadata; fills e on success
+bool storeRebuildParseEntry(MemBuf &buf, StoreEntry &e, cache_key *key, struct _store_rebuild_data &counts, uint64_t expectedSize);
 /// checks whether the loaded entry should be kept; updates counters
-SQUIDCEXTERN bool storeRebuildKeepEntry(const StoreEntry &e, const cache_key *key, struct _store_rebuild_data &counts);
+bool storeRebuildKeepEntry(const StoreEntry &e, const cache_key *key, struct _store_rebuild_data &counts);
 
 
 /*
@@ -198,6 +198,7 @@ mem_hdr::internalAppend(const char *data, int len)
 mem_node *
 mem_hdr::getBlockContainingLocation (int64_t location) const
 {
+    // Optimize: do not create a whole mem_node just to store location
     mem_node target (location);
     target.nodeBuffer.length = 1;
     mem_node *const *result = nodes.find (&target, NodeCompare);
@@ -46,6 +46,7 @@
 #include "mem_node.h"
 #include "StoreMeta.h"
 #include "SwapDir.h"
+#include "StoreIOState.h"
 #if USE_DELAY_POOLS
 #include "DelayPools.h"
 #endif
@@ -526,7 +527,20 @@ StoreEntry::unlock()
     assert(storePendingNClients(this) == 0);
 
     if (EBIT_TEST(flags, RELEASE_REQUEST))
+    {
         this->release();
+        return 0;
+    }
+
+    // XXX: Rock store specific: since each SwapDir controls the index,
+    // unlocked entries should not stay in the global store_table
+    if (fileno >= 0) {
+        Store::Root().dereference(*this);
+        debugs(20, 5, HERE << "destroying unlocked entry: " << this << ' ' << *this);
+        setMemStatus(NOT_IN_MEMORY);
+        destroyStoreEntry(static_cast<hash_link *>(this));
+        return 0;
+    }
     else if (keepInMemory()) {
         Store::Root().dereference(*this);
         setMemStatus(IN_MEMORY);
@@ -1100,16 +1114,6 @@ StoreEntry::abort()
 
     store_status = STORE_OK;
 
-    /*
-     * We assign an object length here.  The only other place we assign
-     * the object length is in storeComplete()
-     */
-    /* RBC: What do we need an object length for? we've just aborted the
-     * request, the request is private and negatively cached. Surely
-     * the object length is inappropriate to set.
-     */
-    mem_obj->object_sz = mem_obj->endOffset();
-
     /* Notify the server side */
 
     /*
@@ -1133,8 +1137,8 @@ StoreEntry::abort()
     /* Notify the client side */
     invokeHandlers();
 
-    /* Close any swapout file */
-    swapOutFileClose();
+    // abort swap out, invalidating what was created so far (release follows)
+    swapOutFileClose(StoreIOState::writerGone);
 
     unlock();       /* unlock */
 }
@@ -1825,49 +1829,15 @@ StoreEntry::replaceHttpReply(HttpReply *rep)
 char const *
 StoreEntry::getSerialisedMetaData()
 {
-    const size_t swap_hdr_sz0 = storeSwapMetaSize(this);
-    assert (swap_hdr_sz0 >= 0);
-    mem_obj->swap_hdr_sz = (size_t) swap_hdr_sz0;
-    // now we can use swap_hdr_sz to calculate swap_file_sz
-    // so that storeSwapMetaBuild/Pack can pack corrent swap_file_sz
-    swap_file_sz = objectLen() + mem_obj->swap_hdr_sz;
-
     StoreMeta *tlv_list = storeSwapMetaBuild(this);
     int swap_hdr_sz;
     char *result = storeSwapMetaPack(tlv_list, &swap_hdr_sz);
-    assert(static_cast<int>(swap_hdr_sz0) == swap_hdr_sz);
     storeSwapTLVFree(tlv_list);
+    assert (swap_hdr_sz >= 0);
+    mem_obj->swap_hdr_sz = (size_t) swap_hdr_sz;
     return result;
 }
 
-/*
- * Calculate TLV list size for a StoreEntry
- * XXX: Must match the actual storeSwapMetaBuild result size
- */
-size_t
-storeSwapMetaSize(const StoreEntry * e)
-{
-    size_t size = 0;
-    ++size; // STORE_META_OK
-    size += sizeof(int); // size of header to follow
-
-    const size_t pfx = sizeof(char) + sizeof(int); // in the start of list entries
-
-    size += pfx + SQUID_MD5_DIGEST_LENGTH;
-    size += pfx + STORE_HDR_METASIZE;
-    size += pfx + strlen(e->url()) + 1;
-
-    // STORE_META_OBJSIZE
-    if (e->objectLen() >= 0)
-        size += pfx + sizeof(int64_t);
-
-    if (const char *vary = e->mem_obj->vary_headers)
-        size += pfx + strlen(vary) + 1;
-
-    debugs(20, 3, "storeSwapMetaSize(" << e->url() << "): " << size);
-    return size;
-}
-
 bool
 StoreEntry::swapoutPossible()
 {
@@ -1877,15 +1847,51 @@ StoreEntry::swapoutPossible()
 
     if (EBIT_TEST(flags, ENTRY_ABORTED)) {
         assert(EBIT_TEST(flags, RELEASE_REQUEST));
-        swapOutFileClose();
+        // StoreEntry::abort() already closed the swap out file, if any
         return false;
     }
 
+    // if we decided that swapout is possible, do not repeat same checks
+    // TODO: do not repeat any checks if we decided that swapout is impossible
+    if (swap_status != SWAPOUT_NONE) {
+        debugs(20, 3, "storeSwapOut: already started");
+        return true;
+    }
+
     if (EBIT_TEST(flags, ENTRY_SPECIAL)) {
         debugs(20, 3, "storeSwapOut: " << url() << " SPECIAL");
         return false;
     }
 
+    // check cache_dir max-size limit if all cache_dirs have it
+    if (store_maxobjsize >= 0) {
+        assert(mem_obj);
+
+        // TODO: add estimated store metadata size to be conservative
+
+        // use guaranteed maximum if it is known
+        const int64_t expectedEnd = mem_obj->expectedReplySize();
+        debugs(20, 7, "storeSwapOut: expectedEnd = " << expectedEnd);
+        if (expectedEnd > store_maxobjsize) {
+            debugs(20, 3, "storeSwapOut: will not fit: " << expectedEnd <<
+                " > " << store_maxobjsize);
+            return false; // known to outgrow the limit eventually
+        }
+        if (expectedEnd < 0) {
+            debugs(20, 3, "storeSwapOut: wait for more info: " <<
+                store_maxobjsize);
+            return false; // may fit later, but will be rejected now
+        }
+
+        // use current minimum (always known)
+        const int64_t currentEnd = mem_obj->endOffset();
+        if (currentEnd > store_maxobjsize) {
+            debugs(20, 3, "storeSwapOut: does not fit: " << currentEnd <<
+                " > " << store_maxobjsize);
+            return false; // already does not fit and may only get bigger
+        }
+    }
+
     return true;
 }
 
@@ -591,9 +591,14 @@ store_client::unpackHeader(char const *buf, ssize_t len)
 
     storeSwapTLVFree(tlv_list);
 
+    assert(swap_hdr_sz >= 0);
+    assert(entry->swap_file_sz > 0);
+    assert(entry->swap_file_sz >= static_cast<uint64_t>(swap_hdr_sz));
     entry->mem_obj->swap_hdr_sz = swap_hdr_sz;
     entry->mem_obj->object_sz = entry->swap_file_sz - swap_hdr_sz;
-
+    debugs(90, 5, "store_client::unpackHeader: swap_file_sz=" <<
+           entry->swap_file_sz << "( " << swap_hdr_sz << " + " <<
+           entry->mem_obj->object_sz << ")");
 }
 
 void
@@ -696,7 +701,7 @@ storeUnregister(store_client * sc, StoreEntry * e, void *data)
         e->swapOut();
 
     if (sc->swapin_sio != NULL) {
-        storeClose(sc->swapin_sio);
+        storeClose(sc->swapin_sio, StoreIOState::readerDone);
         sc->swapin_sio = NULL;
         statCounter.swap.ins++;
     }
@@ -186,7 +186,8 @@ storeDirSelectSwapDirRoundRobin(const StoreEntry * e)
     int load;
     RefCount<SwapDir> sd;
 
-    ssize_t objsize = e->objectLen();
+    // e->objectLen() is negative at this point when we are still STORE_PENDING
+    ssize_t objsize = e->mem_obj->expectedReplySize();
     if (objsize != -1)
         objsize += e->mem_obj->swap_hdr_sz;
 
@@ -196,18 +197,9 @@ storeDirSelectSwapDirRoundRobin(const StoreEntry * e)
 
         sd = dynamic_cast<SwapDir *>(INDEXSD(dirn));
 
-        if (sd->flags.read_only)
+        if (!sd->canStore(*e, objsize, load))
             continue;
 
-        if (sd->cur_size > sd->max_size)
-            continue;
-
-        if (!sd->objectSizeIsAcceptable(objsize))
-            continue;
-
-        /* check for error or overload condition */
-        load = sd->canStore(*e);
-
         if (load < 0 || load > 1000) {
             continue;
         }
@@ -234,7 +226,6 @@ storeDirSelectSwapDirRoundRobin(const StoreEntry * e)
 static int
 storeDirSelectSwapDirLeastLoad(const StoreEntry * e)
 {
-    ssize_t objsize;
     ssize_t most_free = 0, cur_free;
     ssize_t least_objsize = -1;
     int least_load = INT_MAX;
@@ -243,28 +234,20 @@ storeDirSelectSwapDirLeastLoad(const StoreEntry * e)
     int i;
     RefCount<SwapDir> SD;
 
-    /* Calculate the object size */
-    objsize = e->objectLen();
+    // e->objectLen() is negative at this point when we are still STORE_PENDING
+    ssize_t objsize = e->mem_obj->expectedReplySize();
 
     if (objsize != -1)
         objsize += e->mem_obj->swap_hdr_sz;
 
     for (i = 0; i < Config.cacheSwap.n_configured; i++) {
         SD = dynamic_cast<SwapDir *>(INDEXSD(i));
         SD->flags.selected = 0;
-        load = SD->canStore(*e);
-
-        if (load < 0 || load > 1000) {
-            continue;
-        }
-
-        if (!SD->objectSizeIsAcceptable(objsize))
-            continue;
 
-        if (SD->flags.read_only)
+        if (!SD->canStore(*e, objsize, load))
             continue;
 
-        if (SD->cur_size > SD->max_size)
+        if (load < 0 || load > 1000)
             continue;
 
         if (load > least_load)
@@ -15,31 +15,23 @@ StoreIOState::Pointer
 storeCreate(StoreEntry * e, StoreIOState::STFNCB * file_callback, StoreIOState::STIOCB * close_callback, void *callback_data)
 {
     assert (e);
-    ssize_t objsize;
-    sdirno dirn;
-    RefCount<SwapDir> SD;
 
     store_io_stats.create.calls++;
-    /* This is just done for logging purposes */
-    objsize = e->objectLen();
-
-    if (objsize != -1)
-        objsize += e->mem_obj->swap_hdr_sz;
 
     /*
      * Pick the swapdir
      * We assume that the header has been packed by now ..
      */
-    dirn = storeDirSelectSwapDir(e);
+    const sdirno dirn = storeDirSelectSwapDir(e);
 
     if (dirn == -1) {
-        debugs(20, 2, "storeCreate: no valid swapdirs for this object");
+        debugs(20, 2, "storeCreate: no swapdirs for " << *e);
         store_io_stats.create.select_fail++;
         return NULL;
     }
 
-    debugs(20, 2, "storeCreate: Selected dir '" << dirn << "' for obj size '" << objsize << "'");
-    SD = dynamic_cast<SwapDir *>(INDEXSD(dirn));
+    debugs(20, 2, "storeCreate: Selected dir " << dirn << " for " << *e);
+    SwapDir *SD = dynamic_cast<SwapDir *>(INDEXSD(dirn));
 
     /* Now that we have a fs to use, call its storeCreate function */
     StoreIOState::Pointer sio = SD->createStoreIO(*e, file_callback, close_callback, callback_data);
@@ -63,7 +55,7 @@ storeOpen(StoreEntry * e, StoreIOState::STFNCB * file_callback, StoreIOState::ST
 }
 
 void
-storeClose(StoreIOState::Pointer sio)
+storeClose(StoreIOState::Pointer sio, int how)
 {
     if (sio->flags.closing) {
         debugs(20,3,HERE << "storeClose: flags.closing already set, bailing");
@@ -72,8 +64,8 @@ storeClose(StoreIOState::Pointer sio)
 
     sio->flags.closing = 1;
 
-    debugs(20,3,HERE << "storeClose: calling sio->close()");
-    sio->close();
+    debugs(20,3,HERE << "storeClose: calling sio->close(" << how << ")");
+    sio->close(how);
 }
 
 void
@@ -286,24 +286,34 @@ struct InitStoreEntry : public unary_function<StoreMeta, void> {
 };
 
 bool
-storeRebuildLoadEntry(int fd, StoreEntry &tmpe, cache_key *key,
-                      struct _store_rebuild_data &counts, uint64_t expectedSize)
+storeRebuildLoadEntry(int fd, int diskIndex, MemBuf &buf,
+                      struct _store_rebuild_data &counts)
 {
     if (fd < 0)
         return false;
 
-    char hdr_buf[SM_PAGE_SIZE];
+    assert(buf.hasSpace()); // caller must allocate
 
-    ++counts.scancount;
+    const int len = FD_READ_METHOD(fd, buf.space(), buf.spaceSize());
     statCounter.syscalls.disk.reads++;
-    int len;
-    if ((len = FD_READ_METHOD(fd, hdr_buf, SM_PAGE_SIZE)) < 0) {
-        debugs(47, 1, HERE << "failed to read swap entry meta data: " << xstrerror());
+    if (len < 0) {
+        const int xerrno = errno;
+        debugs(47, 1, "cache_dir[" << diskIndex << "]: " <<
+            "failed to read swap entry meta data: " << xstrerr(xerrno));
         return false;
     }
 
+    buf.appended(len);
+    return true;
+}
+
+bool
+storeRebuildParseEntry(MemBuf &buf, StoreEntry &tmpe, cache_key *key,
+                       struct _store_rebuild_data &counts,
+                       uint64_t expectedSize)
+{
     int swap_hdr_len = 0;
-    StoreMetaUnpacker aBuilder(hdr_buf, len, &swap_hdr_len);
+    StoreMetaUnpacker aBuilder(buf.content(), buf.contentSize(), &swap_hdr_len);
     if (aBuilder.isBufferZero()) {
         debugs(47,5, HERE << "skipping empty record.");
         return false;
@@ -320,6 +330,8 @@ storeRebuildLoadEntry(int fd, StoreEntry &tmpe, cache_key *key,
         return false;
     }
 
+    // TODO: consume parsed metadata?
+
     debugs(47,7, HERE << "successful swap meta unpacking");
     memset(key, '\0', SQUID_MD5_DIGEST_LENGTH);
 
@@ -346,6 +358,10 @@ storeRebuildLoadEntry(int fd, StoreEntry &tmpe, cache_key *key,
                    tmpe.swap_file_sz << "!=" << expectedSize);
             return false;
         }
+    } else
+    if (tmpe.swap_file_sz <= 0) {
+        debugs(47, 1, HERE << "missing swap entry size: " << tmpe);
+        return false;
     }
 
     if (EBIT_TEST(tmpe.flags, KEY_PRIVATE)) {
@@ -61,8 +61,8 @@ storeSwapMetaBuild(StoreEntry * e)
     tlv **T = &TLV;
     const char *url;
     const char *vary;
-    const int64_t objsize = e->objectLen();
     assert(e->mem_obj != NULL);
+    const int64_t objsize = e->mem_obj->expectedReplySize();
     assert(e->swap_status == SWAPOUT_WRITING);
     url = e->url();
     debugs(20, 3, "storeSwapMetaBuild: " << url  );
@@ -85,6 +85,9 @@ storeSwapOutStart(StoreEntry * e)
 
     if (sio == NULL) {
         e->swap_status = SWAPOUT_NONE;
+        // our caller thinks SWAPOUT_NONE means swapping out has not started
+        // yet so we better release here to avoid being called again and again
+        e->releaseRequest();
         delete c;
         xfree((char*)buf);
         storeLog(STORE_LOG_SWAPOUTFAIL, e);
@@ -126,22 +129,16 @@ doPages(StoreEntry *anEntry)
     MemObject *mem = anEntry->mem_obj;
 
     do {
-        /*
-         * Evil hack time.
-         * We are paging out to disk in page size chunks. however, later on when
-         * we update the queue position, we might not have a page (I *think*),
-         * so we do the actual page update here.
-         */
+        // find the page containing the first byte we have not swapped out yet
+        mem_node *page =
+            mem->data_hdr.getBlockContainingLocation(mem->swapout.queue_offset);
 
-        if (mem->swapout.memnode == NULL) {
-            /* We need to swap out the first page */
-            mem->swapout.memnode = const_cast<mem_node *>(mem->data_hdr.start());
-        } else {
-            /* We need to swap out the next page */
-            /* 20030636 RBC - we don't have ->next anymore.
-             * But we do have the next location */
-            mem->swapout.memnode = mem->data_hdr.getBlockContainingLocation (mem->swapout.memnode->end());
-        }
+        if (!page)
+            return; // wait for more data to become available
+
+        // memNodeWriteComplete() and absence of buffer offset math below
+        // imply that we always write from the very beginning of the page
+        assert(page->start() == mem->swapout.queue_offset);
 
         /*
          * Get the length of this buffer. We are assuming(!) that the buffer
@@ -151,7 +148,7 @@ doPages(StoreEntry *anEntry)
          * but we can look at this at a later date or whenever the code results
          * in bad swapouts, whichever happens first. :-)
          */
-        ssize_t swap_buf_len = mem->swapout.memnode->nodeBuffer.length;
+        ssize_t swap_buf_len = page->nodeBuffer.length;
 
         debugs(20, 3, "storeSwapOut: swap_buf_len = " << swap_buf_len);
 
@@ -162,7 +159,7 @@ doPages(StoreEntry *anEntry)
         mem->swapout.queue_offset += swap_buf_len;
 
         storeIOWrite(mem->swapout.sio,
-                     mem->data_hdr.NodeGet(mem->swapout.memnode),
+                     mem->data_hdr.NodeGet(page),
                      swap_buf_len,
                      -1,
                      memNodeWriteComplete);
@@ -195,13 +192,17 @@ StoreEntry::swapOut()
     if (!swapoutPossible())
         return;
 
+    // Aborted entries have STORE_OK, but swapoutPossible rejects them. Thus,
+    // store_status == STORE_OK below means we got everything we wanted.
+
     debugs(20, 7, HERE << "storeSwapOut: mem->inmem_lo = " << mem_obj->inmem_lo);
     debugs(20, 7, HERE << "storeSwapOut: mem->endOffset() = " << mem_obj->endOffset());
     debugs(20, 7, HERE << "storeSwapOut: swapout.queue_offset = " << mem_obj->swapout.queue_offset);
 
     if (mem_obj->swapout.sio != NULL)
         debugs(20, 7, "storeSwapOut: storeOffset() = " << mem_obj->swapout.sio->offset()  );
 
+    // buffered bytes we have not swapped out yet
     int64_t swapout_maxsize = mem_obj->endOffset() - mem_obj->swapout.queue_offset;
 
     assert(swapout_maxsize >= 0);
@@ -210,25 +211,29 @@ StoreEntry::swapOut()
 
     debugs(20, 7, HERE << "storeSwapOut: lowest_offset = " << lowest_offset);
 
-    /*
-     * Grab the swapout_size and check to see whether we're going to defer
-     * the swapout based upon size
-     */
-    if ((store_status != STORE_OK) && (swapout_maxsize < store_maxobjsize)) {
-        /*
-         * NOTE: the store_maxobjsize here is the max of optional
-         * max-size values from 'cache_dir' lines.  It is not the
-         * same as 'maximum_object_size'.  By default, store_maxobjsize
-         * will be set to -1.  However, I am worried that this
-         * deferance may consume a lot of memory in some cases.
-         * It would be good to make this decision based on reply
-         * content-length, rather than wait to accumulate huge
-         * amounts of object data in memory.
-         */
-        debugs(20, 5, "storeSwapOut: Deferring starting swapping out");
-        return;
+    // Check to see whether we're going to defer the swapout based upon size
+    if (store_status != STORE_OK) {
+        const int64_t expectedSize = mem_obj->expectedReplySize();
+        const int64_t maxKnownSize = expectedSize < 0 ?
+            swapout_maxsize : expectedSize;
+        debugs(20, 7, HERE << "storeSwapOut: maxKnownSize= " << maxKnownSize);
+
+        if (maxKnownSize < store_maxobjsize) {
+            /*
+             * NOTE: the store_maxobjsize here is the max of optional
+             * max-size values from 'cache_dir' lines.  It is not the
+             * same as 'maximum_object_size'.  By default, store_maxobjsize
+             * will be set to -1.  However, I am worried that this
+             * deferance may consume a lot of memory in some cases.
+             * Should we add an option to limit this memory consumption?
+             */
+            debugs(20, 5, "storeSwapOut: Deferring swapout start for " <<
+                (store_maxobjsize - maxKnownSize) << " bytes");
+            return;
+         }
     }
 
+// TODO: it is better to trim as soon as we swap something out, not before
     trimMemory();
 #if SIZEOF_OFF_T <= 4
 
@@ -247,11 +252,13 @@ StoreEntry::swapOut()
 
     debugs(20, 7, "storeSwapOut: swapout_size = " << swapout_maxsize);
 
-    if (swapout_maxsize == 0) {
-        if (store_status == STORE_OK)
-            swapOutFileClose();
-
-        return;			/* Nevermore! */
+    if (swapout_maxsize == 0) { // swapped everything we got
+        if (store_status == STORE_OK) { // got everything we wanted
+            assert(mem_obj->object_sz >= 0);
+            swapOutFileClose(StoreIOState::wroteAll);
+        }
+        // else need more data to swap out
+        return;
     }
 
     if (store_status == STORE_PENDING) {
@@ -296,22 +303,23 @@ StoreEntry::swapOut()
          * to the filesystem at this point because storeSwapOut() is
          * not going to be called again for this entry.
          */
+        assert(mem_obj->object_sz >= 0);
         assert(mem_obj->endOffset() == mem_obj->swapout.queue_offset);
-        swapOutFileClose();
+        swapOutFileClose(StoreIOState::wroteAll);
     }
 }
 
 void
-StoreEntry::swapOutFileClose()
+StoreEntry::swapOutFileClose(int how)
 {
     assert(mem_obj != NULL);
-    debugs(20, 3, "storeSwapOutFileClose: " << getMD5Text());
+    debugs(20, 3, "storeSwapOutFileClose: " << getMD5Text() << " how=" << how);
     debugs(20, 3, "storeSwapOutFileClose: sio = " << mem_obj->swapout.sio.getRaw());
 
     if (mem_obj->swapout.sio == NULL)
         return;
 
-    storeClose(mem_obj->swapout.sio);
+    storeClose(mem_obj->swapout.sio, how);
 }
 
 static void
@@ -324,7 +332,8 @@ storeSwapOutFileClosed(void *data, int errflag, StoreIOState::Pointer self)
     assert(e->swap_status == SWAPOUT_WRITING);
     cbdataFree(c);
 
-    if (errflag) {
+    // if object_size is still unknown, the entry was probably aborted
+    if (errflag || e->objectLen() < 0) {
         debugs(20, 2, "storeSwapOutFileClosed: dirno " << e->swap_dirn << ", swapfile " <<
                std::hex << std::setw(8) << std::setfill('0') << std::uppercase <<
                e->swap_filen << ", errflag=" << errflag);
@@ -348,9 +357,9 @@ storeSwapOutFileClosed(void *data, int errflag, StoreIOState::Pointer self)
         debugs(20, 3, "storeSwapOutFileClosed: SwapOut complete: '" << e->url() << "' to " <<
                e->swap_dirn  << ", " << std::hex << std::setw(8) << std::setfill('0') <<
                std::uppercase << e->swap_filen);
-        debugs(20, 3, "storeSwapOutFileClosed: Should be:" <<
-               e->swap_file_sz << " = " << e->objectLen() << " + " <<
-               mem->swap_hdr_sz);
+        debugs(20, 5, HERE << "swap_file_sz = " <<
+               e->objectLen() << " + " << mem->swap_hdr_sz);
+        assert(e->objectLen() >= 0); // we checked that above
         e->swap_file_sz = e->objectLen() + mem->swap_hdr_sz;
         e->swap_status = SWAPOUT_DONE;
         e->store()->updateSize(e->swap_file_sz, 1);
@@ -23,9 +23,10 @@ void
 TestSwapDir::init()
 {}
 
-int
-TestSwapDir::canStore(const StoreEntry&) const
+bool
+TestSwapDir::canStore(const StoreEntry &, int64_t, int &load) const
 {
+    load = 0;
     return true;
 }
 
@@ -17,7 +17,7 @@ class TestSwapDir : public SwapDir
 
     virtual void reconfigure(int, char*);
     virtual void init();
-    virtual int canStore(const StoreEntry&) const;
+    virtual bool canStore(const StoreEntry &e, int64_t diskSpaceNeeded, int &load) const;
     virtual StoreIOState::Pointer createStoreIO(StoreEntry &, StoreIOState::STFNCB *, StoreIOState::STIOCB *, void *);
     virtual StoreIOState::Pointer openStoreIO(StoreEntry &, StoreIOState::STFNCB *, StoreIOState::STIOCB *, void *);
     virtual void parse(int, char*);
@@ -46,7 +46,7 @@ storeRebuildComplete(struct _store_rebuild_data *dc)
 {}
 
 bool
-storeRebuildLoadEntry(int fd, StoreEntry &tmpe, cache_key *key,
+storeRebuildLoadEntry(MemBuf &buf, StoreEntry &e, cache_key *key,
     struct _store_rebuild_data &counts, uint64_t expectedSize)
 {
     return false;
@@ -38,7 +38,7 @@
 StoreIoStats store_io_stats;
 
 void
-StoreEntry::swapOutFileClose()
+StoreEntry::swapOutFileClose(int)
 {
     fatal ("Not implemented");
 }